{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回忆使用tensorflow构建神经网络的步骤：\n",
    "# 构建计算图---LSTM模型:\n",
    "#     embedding 层\n",
    "#     LSTM 层\n",
    "#     fc 全连接层\n",
    "#     train_op\n",
    "# 训练流程代码：\n",
    "# 数据集封装：\n",
    "#     api: next_batch(batch_size) \n",
    "# 词表封装：\n",
    "#     api: sentence2id(text_sentence): 句子转id\n",
    "# 类别的封装：\n",
    "#    api:  category2id(text_category)\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 设置日志\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_default_params():\n",
    "    \"\"\"get default parameters 原来直接定义成全局变量，这样封装，方便管理\"\"\"\n",
    "    # 如果想得到更好的结果，可以调大下面的值，考虑到笔记本中运行\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_embedding_size = 16,\n",
    "        num_timesteps = 50, # 每个batch是定长的\n",
    "        \n",
    "        num_filters = 128, # 卷积核数目大小\n",
    "        num_kernel_size = 3, # 卷积核是3 x 3的\n",
    "#         num_lstm_nodes = [32,32],\n",
    "#         num_lstm_layers = 2,\n",
    "        num_fc_nodes = 32,\n",
    "        batch_size = 100,\n",
    "#         clip_lstm_grads = 1.0, # 用来控制lstm梯度大小， 用来防止梯度爆炸, 如果超过某一上限，则设定为1.0\n",
    "        learning_rate = 0.001,\n",
    "        num_word_threadhold = 10, # 用来选择只有词频超过10才纳入进来\n",
    "    )\n",
    "\n",
    "hps = get_default_params()  # 参数值可以通过hps.xxx来访问\n",
    "train_file = './cnews_data/cnews.train.seg.txt'\n",
    "val_file = './cnews_data/cnews.val.seg.txt'\n",
    "test_file = './cnews_data/cnews.test.seg.txt'\n",
    "vocab_file = './cnews_data/cnews.vocab.txt'\n",
    "category_file = './cnews_data/cnews.category.txt'\n",
    "output_dir = './cnews_data/run_text_rnn'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab size: 77323\n",
      "INFO:tensorflow:id of 时尚 is 5\n",
      "INFO:tensorflow:num_classes: 10\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, file_name, num_word_threadhold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._num_word_threadhold = num_word_threadhold\n",
    "        self._read_dict(file_name)\n",
    "        \n",
    "    def _read_dict(self, file_name):\n",
    "        with open(file_name, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threadhold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "            \n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "            \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "        \n",
    "    def sentence_to_id(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "    \n",
    "# 测试代码    \n",
    "vocab = Vocab(vocab_file, hps.num_word_threadhold)\n",
    "vocab_size = vocab.size()\n",
    "tf.logging.info('vocab size: %d' % vocab_size)\n",
    "\n",
    "# test_str = '和 也 有 我'\n",
    "# print(vocab.sentence_to_id(test_str))\n",
    "\n",
    "# 打印结果：\n",
    "# INFO:tensorflow:vocab size: 77323\n",
    "# [10, 12, 13, 18]\n",
    "\n",
    "# 类别封装\n",
    "class CategoryDict:\n",
    "    def __init__(self, category_file):\n",
    "        self._category_to_id = {}\n",
    "        with open(category_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "        \n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "        \n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            raise Exception('%s not in our category list.' % category)\n",
    "        return self._category_to_id[category]\n",
    "\n",
    "category_vocab = CategoryDict(category_file)\n",
    "category_name = '时尚'\n",
    "num_classes = category_vocab.size()\n",
    "tf.logging.info('id of %s is %d' % (category_name, category_vocab.category_to_id(category_name)))\n",
    "tf.logging.info('num_classes: %d' % category_vocab.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading data from ./cnews_data/cnews.train.seg.txt\n",
      "INFO:tensorflow:Loading data from ./cnews_data/cnews.val.seg.txt\n",
      "INFO:tensorflow:Loading data from ./cnews_data/cnews.test.seg.txt\n",
      "(array([[  960, 14098,  1946, ...,     0,     0,     0],\n",
      "       [  138,     8, 12720, ...,   198,   116,    71]]), array([6, 4]))\n",
      "(array([[ 2417,   977,  1847, ...,     0,     0,     0],\n",
      "       [23570,    32, 26232, ...,     0,     0,     0]]), array([0, 1]))\n",
      "(array([[ 1524,   531,     2, ...,     0,     0,     0],\n",
      "       [ 5326,  1996, 21121, ...,     0,     0,     0]]), array([5, 6]))\n"
     ]
    }
   ],
   "source": [
    "# 封装数据集\n",
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        # matrix\n",
    "        self._inputs = []\n",
    "        # vector\n",
    "        self._outputs = []\n",
    "        self._indicator = 0 # 当前读取到的位置\n",
    "        self._parse_file(filename)\n",
    "        \n",
    "    def _parse_file(self, filename):\n",
    "        tf.logging.info('Loading data from %s' % filename)\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            id_label = self._category_vocab.category_to_id(label)\n",
    "            id_words = self._vocab.sentence_to_id(content) # 它是一个列表\n",
    "            # 截取单词\n",
    "            id_words = id_words[0:self._num_timesteps]\n",
    "            # 对于不足num_timesteps的要补齐\n",
    "            num_padding = self._num_timesteps - len(id_words)\n",
    "            id_words = id_words + [self._vocab.unk for i in range(num_padding)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "            \n",
    "        # 将输入转换成矩阵\n",
    "        self._inputs = np.asarray(self._inputs, dtype=np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype=np.int32)\n",
    "        # 随机化，让inputs, outputs同步随机化\n",
    "        self._random_shuffle()\n",
    "        \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "#         tf.logging.info('the type of p is %s' % type(p))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        api: 获取下一个批处理块\n",
    "        - batch_size: 批处理块尺寸\n",
    "        \"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        # 表示已经到结尾了\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        # 如果还比输入大，则抛出异常，批处理块尺寸太大了\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Exception('batch size: %d is too large. ' % batch_size)\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator : end_indicator]\n",
    "        batch_outputs = self._outputs[self._indicator : end_indicator]\n",
    "        \n",
    "        return batch_inputs, batch_outputs\n",
    "    \n",
    "train_dataset = TextDataSet(train_file, vocab, category_vocab, hps.num_timesteps)\n",
    "val_dataset = TextDataSet(val_file, vocab, category_vocab, hps.num_timesteps)       \n",
    "test_dataset = TextDataSet(test_file, vocab, category_vocab, hps.num_timesteps)\n",
    "\n",
    "# inputs是一个2 x 50的矩阵(里面是词语的id)，outputs是一个向量（数组）(里面是类别的id)\n",
    "print(train_dataset.next_batch(2))\n",
    "print(val_dataset.next_batch(2))\n",
    "print(test_dataset.next_batch(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-f4edf872237e>:80: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From c:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From c:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "# 构建计算图\n",
    "def create_model(hps, vocab_size, num_classes):\n",
    "    \"\"\"\n",
    "    args:\n",
    "    - hps: 参数\n",
    "    - vocab_size: 词表大小\n",
    "    - num_classes: 类别大小\n",
    "    \"\"\"\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32, (batch_size, num_timesteps)) # 矩阵(100 x 16)\n",
    "    outputs = tf.placeholder(tf.int32, (batch_size, ))  #向量\n",
    "    # 讲resnet时讲到，就表示我keep多少值，丢掉的就是1-keep的值\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    # 保存当前训练到多少步\n",
    "    global_step = tf.Variable(tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "    \n",
    "    # embedding 层构建开始==================================================\n",
    "    # 构建一个初始化函数\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0) # 在-1.0~1.0之间进行均匀初始化\n",
    "    # tf.variable_scope跟name_scope不同在于：前者可以指定一个初始化器\n",
    "    with tf.variable_scope('embedding', initializer=embedding_initializer):\n",
    "        \"\"\"有词表个向量，每个向量都是embedding size的矩阵, 长：词表数目，宽：embedding size\"\"\"\n",
    "        # tf.get_varitable 如果变量存在就取，否则新建此变量\n",
    "        embeddings = tf.get_variable(\n",
    "            'embedding',\n",
    "            [vocab_size, hps.num_embedding_size],\n",
    "            tf.float32\n",
    "        )\n",
    "        # 将输入转换成embedding输入\n",
    "        # [1, 10, 7] -> [embeddings[1], embeddings[10], embeddings[7]]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    # embedding 层构建结束==================================================\n",
    "    \n",
    "    # 构建卷积层\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size + hps.num_filters) / 3.0\n",
    "    cnn_init = tf.random_uniform_initializer(-scale, scale)\n",
    "\n",
    "    with tf.variable_scope('cnn', initializer = cnn_init):\n",
    "        # 构建1维卷积\n",
    "        # embed_inputs: [batch_size, timesteps, embed_size]\n",
    "        # conv1d: [batch_size, timesteps, num_filters]\n",
    "        conv1d = tf.keras.layers.Conv1D(\n",
    "            hps.num_filters,\n",
    "            hps.num_kernel_size,\n",
    "            activation = tf.nn.relu,\n",
    "        )(embed_inputs)\n",
    "        # 构建pooling层\n",
    "        global_maxpooling = tf.reduce_max(conv1d, axis=[1]) # timesteps这一维度\n",
    "    \n",
    "    \"\"\"\n",
    "    # lstm层构建开始========================================================\n",
    "    # 输入层的大小 + 输出层的大小 再做开方的三分之一再被一除\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_initilizer = tf.random_normal_initializer(-scale, scale)  # 可以换成自己的initializer\n",
    "    # 构建两层lstm   \n",
    "    with tf.variable_scope('lstm_nn', initializer = lstm_initilizer):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "                hps.num_lstm_nodes[i],\n",
    "                state_is_tuple = True)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell,\n",
    "                output_keep_prob = keep_prob)\n",
    "            cells.append(cell)\n",
    "        \n",
    "        # 合并cell, 第一个cell的输出是第二个cell的输入\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        # 中间状态\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        # rnn_outputs: 三维数组：[batch_size, num_timesteps, lstm_outpus[-1]]\n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(cell, embed_inputs, initial_state = initial_state)\n",
    "        # last为lstm最后的输出\n",
    "        last = rnn_outputs[:, -1, :]\n",
    "    # lstm层构建结束======================================================== \n",
    "    \"\"\"\n",
    "    # fc层构建开始==========================================================\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        fc1 = tf.keras.layers.Dense(\n",
    "                    hps.num_fc_nodes,\n",
    "                    activation = tf.nn.relu,\n",
    "                    name = 'fc1')(global_maxpooling)\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        logits = tf.keras.layers.Dense(\n",
    "                    num_classes,\n",
    "                    name = 'fc2')(fc1_dropout)    \n",
    "    # fc层构建结束==========================================================\n",
    "    \n",
    "    # 计算损失函数\n",
    "    with tf.name_scope('metrics'):\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                            logits = logits, labels = outputs)\n",
    "        loss = tf.reduce_mean(softmax_loss)\n",
    "        # 计算预测值 [0, 1, 5, 2, 4] -> argmax: 2 第2个位置上的值最大\n",
    "        y_pred = tf.argmax(tf.nn.softmax(logits), 1, output_type = tf.int32)\n",
    "        # 判断是否预测正确：bool\n",
    "        correct_pred = tf.equal(outputs, y_pred)\n",
    "        # 计算正确率 tf.cast将布尔转换成浮点型\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "    # 计算train_op\n",
    "    with tf.name_scope('train_op'):\n",
    "        train_op = tf.train.AdadeltaOptimizer(hps.learning_rate).minimize(loss, global_step=global_step)\n",
    "        \"\"\"\n",
    "        tvars = tf.trainable_variables() # 获得所有可以训练的变量\n",
    "        for var in tvars:\n",
    "            tf.logging.info('variable name: %s' % var.name)\n",
    "        # 对梯度做截断\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        # 将截断后的梯度应用到所有变量上去\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "                    zip(grads,tvars), global_step = global_step)\n",
    "        \"\"\"\n",
    "    return (\n",
    "        (inputs, outputs, keep_prob), # 输入 placeholders\n",
    "        (loss, accuracy), # 输出\n",
    "        (train_op, global_step)\n",
    "    )\n",
    "\n",
    "\n",
    "placeholders, metrics, others = create_model(hps, vocab_size, num_classes)\n",
    "\n",
    "inputs, outputs, keep_prob = placeholders\n",
    "loss, accuracy = metrics\n",
    "train_op, global_step = others\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:     0, loss: 2.616, accuracy: 0.060\n",
      "INFO:tensorflow:Step:   100, loss: 2.575, accuracy: 0.090\n",
      "INFO:tensorflow:Step:   200, loss: 2.592, accuracy: 0.070\n",
      "INFO:tensorflow:Step:   300, loss: 2.543, accuracy: 0.120\n",
      "INFO:tensorflow:Step:   400, loss: 2.487, accuracy: 0.100\n",
      "INFO:tensorflow:Step:   500, loss: 2.562, accuracy: 0.090\n",
      "INFO:tensorflow:Step:   600, loss: 2.518, accuracy: 0.080\n",
      "INFO:tensorflow:Step:   700, loss: 2.507, accuracy: 0.090\n",
      "INFO:tensorflow:Step:   800, loss: 2.422, accuracy: 0.100\n",
      "INFO:tensorflow:Step:   900, loss: 2.464, accuracy: 0.090\n",
      "INFO:tensorflow:Step:  1000, loss: 2.476, accuracy: 0.140\n",
      "INFO:tensorflow:Step:  1100, loss: 2.441, accuracy: 0.100\n",
      "INFO:tensorflow:Step:  1200, loss: 2.427, accuracy: 0.110\n",
      "INFO:tensorflow:Step:  1300, loss: 2.459, accuracy: 0.120\n",
      "INFO:tensorflow:Step:  1400, loss: 2.388, accuracy: 0.110\n",
      "INFO:tensorflow:Step:  1500, loss: 2.418, accuracy: 0.120\n",
      "INFO:tensorflow:Step:  1600, loss: 2.390, accuracy: 0.080\n",
      "INFO:tensorflow:Step:  1700, loss: 2.357, accuracy: 0.110\n",
      "INFO:tensorflow:Step:  1800, loss: 2.350, accuracy: 0.110\n",
      "INFO:tensorflow:Step:  1900, loss: 2.353, accuracy: 0.120\n",
      "INFO:tensorflow:Step:  2000, loss: 2.341, accuracy: 0.130\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-85033e28101b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m                               \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                               \u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                               \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_keep_prob_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m                           })\n\u001b[0;32m     23\u001b[0m         \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练流程部分\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "train_keep_prob_value = 0.8 # \n",
    "test_keep_prob_value = 1.0 # 做测试的时候不会去做dropout\n",
    "\n",
    "# 定义train多少步\n",
    "num_train_steps = 10000\n",
    "\n",
    "# Train: 100%\n",
    "# Valid: 95.7%\n",
    "# Test:  95.3%\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(num_train_steps):\n",
    "        batch_inputs, batch_labels = train_dataset.next_batch(hps.batch_size)\n",
    "        outputs_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                          feed_dict = {\n",
    "                              inputs: batch_inputs,\n",
    "                              outputs: batch_labels,\n",
    "                              keep_prob: train_keep_prob_value,\n",
    "                          })\n",
    "        loss_val, accuracy_val, _, global_step_val = outputs_val\n",
    "        # 每100次打印\n",
    "        if global_step_val % 100 == 0:\n",
    "            tf.logging.info('Step: %5d, loss: %3.3f, accuracy: %3.3f' % (global_step_val, loss_val,accuracy_val))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
