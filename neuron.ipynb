{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batches.meta', 'data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', 'readme.html', 'test_batch']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "\n",
    "CIFAR_DIR = \"C:\\\\Users\\\\i076453\\\\Downloads\\\\tech\\\\ml\\\\cifar-10-batches-py\"\n",
    "print(os.listdir(CIFAR_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072)\n",
      "(10000,)\n",
      "[3328 2024 8185 ... 2046 9549 6600]\n",
      "(2000, 3072)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\" read data from data file \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = cPickle.load(f, encoding='bytes')\n",
    "        return data[b'data'], data[b'labels']\n",
    "\n",
    "# tensorflow.dataset\n",
    "class CifarData:\n",
    "    def __init__(self, filenames, need_shuffle):\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        for filename in filenames:\n",
    "            data, labels = load_data(filename)\n",
    "            for item, label in zip(data, labels):\n",
    "                if label in [0, 1]:  # 只取0,1两类\n",
    "                    all_data.append(item)\n",
    "                    all_labels.append(label)\n",
    "        self._data = np.vstack(all_data)  # 纵向合并成\n",
    "        self._data = self._data / 127.5 - 1 # 进行归一化 \n",
    "            \n",
    "        self._labels = np.hstack(all_labels) # 横向合并\n",
    "        \n",
    "        print(self._data.shape)\n",
    "         print(self._labels.shape)\n",
    "        \n",
    "        self._num_examples = self._data.shape[0]\n",
    "        self._need_shuffle = need_shuffle\n",
    "        self._indicator = 0\n",
    "        if self._need_shuffle:\n",
    "            self._shuffle_data()\n",
    "        \n",
    "    def _shuffle_data(self):\n",
    "        # [0,1,2,3,4,5] -> [5,3,2,4,0,1]\n",
    "        p = np.random.permutation(self._num_examples) # 取得10000\n",
    "        print(p)\n",
    "        self._data = self._data[p]\n",
    "        self._labels = self._labels[p]\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"return batch_size examples as a batch\"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self._num_examples:\n",
    "            if self._need_shuffle:\n",
    "                self._shuffle_data()\n",
    "                self._indicator = 0\n",
    "                end_indicator = batch_size\n",
    "            else:\n",
    "                raise Exception(\"have no more examples\")\n",
    "        \n",
    "        if end_indicator > self._num_examples:\n",
    "            raise Exception(\"batch size is larger than all examples\")\n",
    "        batch_data = self._data[self._indicator: end_indicator]\n",
    "        batch_labels = self._labels[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "train_filenames = [os.path.join(CIFAR_DIR, 'data_batch_%d' % i) for i in range(1, 6)] \n",
    "test_filenames = [os.path.join(CIFAR_DIR, 'test_batch')]\n",
    "\n",
    "train_data = CifarData(train_filenames, True)\n",
    "# (10000, 3072) data: 5个文件，每个类别5000张图片，只取两类，就是1/5, 也就是10000\n",
    "# (10000,) labels: 它是一个一维数组\n",
    "test_data = CifarData(test_filenames, False)\n",
    "# (2000, 3072) data: 1个文件，取1/5, 也就是2000 \n",
    "# (2000,) labels: \n",
    "\n",
    "# batch_data, batch_labels = train_data.next_batch(10)\n",
    "# print(batch_data)\n",
    "# print(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"Equal:0\", shape=(?, 1), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# 构建tensorflow计算图\n",
    "x = tf.placeholder(tf.float32, [None, 3072]) # data 的placeholder, None表示不确定的\n",
    "# print(x) Tensor(\"Placeholder:0\", shape=(?, 3072), dtype=float32)\n",
    "# y的shape是[None]\n",
    "y = tf.placeholder(tf.int64, [None]) # labels的placeholder\n",
    "# (3072, 1)\n",
    "# print(x.get_shape()[-1]) --> 3072\n",
    "w = tf.get_variable('w', [x.get_shape()[-1], 1],\n",
    "                    initializer=tf.random_normal_initializer(0,1))  # x.get_shape()[-1] 就是3072\n",
    "# (1, )\n",
    "b = tf.get_variable('b', [1], \n",
    "                   initializer=tf.constant_initializer(0.0))\n",
    "# [None, 3072] * [3072, 1] = [None, 1]   matmul 就是 matrix matipule 矩阵相乘\n",
    "y_ = tf.matmul(x, w) + b\n",
    "\n",
    "# [None, 1] 将y 变成一个概率值\n",
    "p_y_1 = tf.nn.sigmoid(y_)\n",
    "\n",
    "# 因为p_y_1于y比较，y的shape跟它的不同， 所以需要做一个reshape\n",
    "# 调用完后，就变成[None, 1]这样一个shape\n",
    "y_reshaped = tf.reshape(y, (-1, 1))\n",
    "\n",
    "# 类型变换,将其与p_y_1类型统一\n",
    "y_reshaped_float = tf.cast(y_reshaped, tf.float32)\n",
    "\n",
    "# 计算损失的目标函数\n",
    "# reduce_mean求平均值， square是求平方\n",
    "loss = tf.reduce_mean(tf.square(y_reshaped_float - p_y_1))\n",
    "\n",
    "# bool \n",
    "predict = p_y_1 > 0.5\n",
    "# [1,0,0,1,0,0,1,1...]  比较后，相等是1，不等是0   （可以理解为：predict为预测值，y_reshaped为真实值）\n",
    "correct_predict = tf.equal(tf.cast(predict, tf.int64), y_reshaped)\n",
    "print(correct_predict)\n",
    "\n",
    "# 准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float64))\n",
    "\n",
    "# 有了loss目标函数和准确率后，再去做一个\n",
    "# AdamOptimizer 是一个梯度下降的变种，1e-3就是10的-3次方0.001 \n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 500, loss: 0.26149, acc: 0.70000\n",
      "[2010 5360 9020 ...   69 1033 2769]\n",
      "[Train] Step: 1000, loss: 0.34989, acc: 0.65000\n",
      "[3239 3081 1811 ... 4501 5396 5401]\n",
      "[Train] Step: 1500, loss: 0.19071, acc: 0.80000\n",
      "[ 870 9866  448 ... 6616 8661 3372]\n",
      "[Train] Step: 2000, loss: 0.34746, acc: 0.65000\n",
      "[8464 6339 2443 ...  693 4173 8772]\n",
      "[Train] Step: 2500, loss: 0.19996, acc: 0.80000\n",
      "[2005 5082 1774 ... 2373 5141 5278]\n",
      "[Train] Step: 3000, loss: 0.20339, acc: 0.80000\n",
      "[7116 8859 8490 ... 9707 3842 6311]\n",
      "[Train] Step: 3500, loss: 0.11651, acc: 0.85000\n",
      "[5379 7410 6082 ... 6041 7307 5923]\n",
      "[Train] Step: 4000, loss: 0.09978, acc: 0.90000\n",
      "[7857 2610 7418 ... 4823 8091 7419]\n",
      "[Train] Step: 4500, loss: 0.19998, acc: 0.80000\n",
      "[9923 7061 3298 ... 2478 7722 7368]\n",
      "[Train] Step: 5000, loss: 0.20210, acc: 0.80000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 5000, acc: 0.79000\n",
      "[1853 1577 1245 ... 8653 1806 2406]\n",
      "[Train] Step: 5500, loss: 0.17492, acc: 0.80000\n",
      "[7490 9368 8080 ... 5381 7129  551]\n",
      "[Train] Step: 6000, loss: 0.39920, acc: 0.60000\n",
      "[3106 9329 7730 ... 5044 4849 5928]\n",
      "[Train] Step: 6500, loss: 0.29238, acc: 0.70000\n",
      "[5233  634 9243 ... 2412   91 5498]\n",
      "[Train] Step: 7000, loss: 0.10368, acc: 0.90000\n",
      "[3311  651 8615 ... 5762 5322 1157]\n",
      "[Train] Step: 7500, loss: 0.19959, acc: 0.80000\n",
      "[ 309 9884 9367 ... 2785 4463  487]\n",
      "[Train] Step: 8000, loss: 0.33264, acc: 0.65000\n",
      "[4242 7803 9502 ... 7316 1924 5300]\n",
      "[Train] Step: 8500, loss: 0.21668, acc: 0.75000\n",
      "[ 227 3336 8169 ... 1617 5986 1383]\n",
      "[Train] Step: 9000, loss: 0.07598, acc: 0.90000\n",
      "[3032 3081 3085 ...  882 7627 7020]\n",
      "[Train] Step: 9500, loss: 0.17688, acc: 0.80000\n",
      "[9934 7916  441 ... 1085 1244 3087]\n",
      "[Train] Step: 10000, loss: 0.10000, acc: 0.90000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 10000, acc: 0.80550\n",
      "[5220  671 4600 ... 9930 8844 2251]\n",
      "[Train] Step: 10500, loss: 0.04997, acc: 0.95000\n",
      "[9429 9754 3076 ... 1683 2809 9564]\n",
      "[Train] Step: 11000, loss: 0.25000, acc: 0.75000\n",
      "[2012 1609 9366 ... 7769 2700 9868]\n",
      "[Train] Step: 11500, loss: 0.24999, acc: 0.75000\n",
      "[1117  747   23 ... 2550 3933 9325]\n",
      "[Train] Step: 12000, loss: 0.10260, acc: 0.90000\n",
      "[8927 8872 1178 ... 2577 1184 1053]\n",
      "[Train] Step: 12500, loss: 0.10000, acc: 0.90000\n",
      "[9279 2877  708 ... 3004  631 1730]\n",
      "[Train] Step: 13000, loss: 0.05001, acc: 0.95000\n",
      "[3515 2979 8461 ... 8546 5436 2534]\n",
      "[Train] Step: 13500, loss: 0.10755, acc: 0.90000\n",
      "[7464 3622 8727 ... 3421 7207 6337]\n",
      "[Train] Step: 14000, loss: 0.29990, acc: 0.70000\n",
      "[2322 9360 3910 ... 6641 3770 1299]\n",
      "[Train] Step: 14500, loss: 0.00000, acc: 1.00000\n",
      "[5602 4422 5374 ... 1401 3377 7984]\n",
      "[Train] Step: 15000, loss: 0.17770, acc: 0.80000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 15000, acc: 0.80550\n",
      "[  74 4429 2322 ... 5898 3874 3064]\n",
      "[Train] Step: 15500, loss: 0.30019, acc: 0.70000\n",
      "[  32 1567 6455 ... 9429 6895 7290]\n",
      "[Train] Step: 16000, loss: 0.05073, acc: 0.95000\n",
      "[7341 2950 4914 ... 7525 4063 6269]\n",
      "[Train] Step: 16500, loss: 0.15022, acc: 0.85000\n",
      "[3066 4163 8780 ... 4055 8149  866]\n",
      "[Train] Step: 17000, loss: 0.15000, acc: 0.85000\n",
      "[9325 1029 6404 ... 4302 1087 6680]\n",
      "[Train] Step: 17500, loss: 0.20391, acc: 0.80000\n",
      "[7852 4818 5604 ... 5769 7941 2449]\n",
      "[Train] Step: 18000, loss: 0.20000, acc: 0.80000\n",
      "[2158 1979 2679 ... 1327 2989 1055]\n",
      "[Train] Step: 18500, loss: 0.06211, acc: 0.95000\n",
      "[ 325 1963 7312 ... 5634 7737 9030]\n",
      "[Train] Step: 19000, loss: 0.09843, acc: 0.90000\n",
      "[  26 8886 2273 ... 9820 9500 5972]\n",
      "[Train] Step: 19500, loss: 0.14990, acc: 0.85000\n",
      "[8988 5668 8110 ...  957 4741 6006]\n",
      "[Train] Step: 20000, loss: 0.25009, acc: 0.75000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 20000, acc: 0.81000\n",
      "[4818 5802 4311 ... 4385   70 9869]\n",
      "[Train] Step: 20500, loss: 0.00000, acc: 1.00000\n",
      "[6312 3244 8278 ... 2867 2690 9925]\n",
      "[Train] Step: 21000, loss: 0.10000, acc: 0.90000\n",
      "[ 505 7806 6670 ... 9370 7477 9727]\n",
      "[Train] Step: 21500, loss: 0.19899, acc: 0.80000\n",
      "[8951 7842 4074 ...  499 4456 9524]\n",
      "[Train] Step: 22000, loss: 0.10000, acc: 0.90000\n",
      "[4006  406 2554 ... 2225  267  111]\n",
      "[Train] Step: 22500, loss: 0.38630, acc: 0.60000\n",
      "[5776 7390  261 ... 9731 1467 8613]\n",
      "[Train] Step: 23000, loss: 0.15858, acc: 0.85000\n",
      "[2342 1998 9673 ... 8236 1937 5411]\n",
      "[Train] Step: 23500, loss: 0.10053, acc: 0.90000\n",
      "[9030 1551 1679 ... 7884 3883 6411]\n",
      "[Train] Step: 24000, loss: 0.20923, acc: 0.80000\n",
      "[3231 4979 2840 ...  733 8100 2329]\n",
      "[Train] Step: 24500, loss: 0.16672, acc: 0.80000\n",
      "[5413 2367 7864 ... 8688 1682 6261]\n",
      "[Train] Step: 25000, loss: 0.17507, acc: 0.80000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 25000, acc: 0.81050\n",
      "[9636 9451 9716 ... 3016 5775 6426]\n",
      "[Train] Step: 25500, loss: 0.24730, acc: 0.75000\n",
      "[7757 4507 7425 ... 7017 5593 6493]\n",
      "[Train] Step: 26000, loss: 0.10278, acc: 0.90000\n",
      "[1939 5804 3147 ... 1254 8584 9799]\n",
      "[Train] Step: 26500, loss: 0.10000, acc: 0.90000\n",
      "[2568 8657 9047 ... 7449 6044 3316]\n",
      "[Train] Step: 27000, loss: 0.14999, acc: 0.85000\n",
      "[ 613 1842 2247 ... 3490 4150 4843]\n",
      "[Train] Step: 27500, loss: 0.25000, acc: 0.75000\n",
      "[2078 9050 5689 ... 7481 3422 8008]\n",
      "[Train] Step: 28000, loss: 0.19993, acc: 0.80000\n",
      "[1969 2603 3932 ... 4123 5663 5181]\n",
      "[Train] Step: 28500, loss: 0.09999, acc: 0.90000\n",
      "[7475 1134 3711 ... 9213 2829 7761]\n",
      "[Train] Step: 29000, loss: 0.05003, acc: 0.95000\n",
      "[5916 2979 9699 ... 5989 9349 3779]\n",
      "[Train] Step: 29500, loss: 0.14004, acc: 0.85000\n",
      "[1717  114 1277 ... 9327  998 1731]\n",
      "[Train] Step: 30000, loss: 0.15000, acc: 0.85000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 30000, acc: 0.81400\n",
      "[9428 5592 2179 ... 1302 5791 4245]\n",
      "[Train] Step: 30500, loss: 0.14935, acc: 0.85000\n",
      "[1963 8726 7577 ... 5746 4231 1115]\n",
      "[Train] Step: 31000, loss: 0.14762, acc: 0.85000\n",
      "[7571 3326 1589 ... 4905 9559 6935]\n",
      "[Train] Step: 31500, loss: 0.11187, acc: 0.90000\n",
      "[4276 2647 5594 ... 1845  405 1935]\n",
      "[Train] Step: 32000, loss: 0.16749, acc: 0.80000\n",
      "[3411 2117 3046 ...  470 6877   33]\n",
      "[Train] Step: 32500, loss: 0.05554, acc: 0.95000\n",
      "[3808 7595 7277 ... 2021 1901 5296]\n",
      "[Train] Step: 33000, loss: 0.05696, acc: 0.95000\n",
      "[5834 6910  611 ... 2568 8200 4808]\n",
      "[Train] Step: 33500, loss: 0.18940, acc: 0.80000\n",
      "[2108 7334 8152 ... 1769 3548 1290]\n",
      "[Train] Step: 34000, loss: 0.11293, acc: 0.90000\n",
      "[4170 5217 7534 ... 7589 2008 9265]\n",
      "[Train] Step: 34500, loss: 0.08905, acc: 0.90000\n",
      "[3722 6400 2388 ... 7036 2342 6480]\n",
      "[Train] Step: 35000, loss: 0.11403, acc: 0.85000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 35000, acc: 0.81300\n",
      "[ 118 6844 5468 ... 3112 4150 9308]\n",
      "[Train] Step: 35500, loss: 0.34990, acc: 0.65000\n",
      "[3632 4962 2110 ... 4963 5702 9232]\n",
      "[Train] Step: 36000, loss: 0.13584, acc: 0.85000\n",
      "[2190 2881 1028 ... 6016 5950  990]\n",
      "[Train] Step: 36500, loss: 0.05024, acc: 0.95000\n",
      "[ 116  132 5501 ... 5989 4604 1946]\n",
      "[Train] Step: 37000, loss: 0.20916, acc: 0.80000\n",
      "[5587 1329 5351 ... 4079 2723 8342]\n",
      "[Train] Step: 37500, loss: 0.05154, acc: 0.95000\n",
      "[1507 5324 3174 ... 6753 5383 3404]\n",
      "[Train] Step: 38000, loss: 0.15670, acc: 0.85000\n",
      "[7263 3569 1938 ... 4580 5497 4658]\n",
      "[Train] Step: 38500, loss: 0.25516, acc: 0.75000\n",
      "[1334 1970 9868 ... 6054 9285 5997]\n",
      "[Train] Step: 39000, loss: 0.10151, acc: 0.90000\n",
      "[9054  768  888 ... 7617 9649 1772]\n",
      "[Train] Step: 39500, loss: 0.10743, acc: 0.90000\n",
      "[9569 5275 3872 ... 3567 9524 2173]\n",
      "[Train] Step: 40000, loss: 0.15469, acc: 0.85000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 40000, acc: 0.81450\n",
      "[3226 3815 9180 ... 5374  754 9081]\n",
      "[Train] Step: 40500, loss: 0.10124, acc: 0.90000\n",
      "[4031 6568  575 ... 3456  202 5654]\n",
      "[Train] Step: 41000, loss: 0.07226, acc: 0.90000\n",
      "[  79 1720 9852 ... 3532  917 2574]\n",
      "[Train] Step: 41500, loss: 0.15015, acc: 0.85000\n",
      "[ 577 8240 1667 ... 3327 1887 5776]\n",
      "[Train] Step: 42000, loss: 0.30331, acc: 0.70000\n",
      "[7450 1176 7204 ... 2178  572 7373]\n",
      "[Train] Step: 42500, loss: 0.25001, acc: 0.75000\n",
      "[2328 4264 1391 ... 2603 3374 4471]\n",
      "[Train] Step: 43000, loss: 0.05441, acc: 0.95000\n",
      "[5438 1061 9914 ... 4845 4337 5978]\n",
      "[Train] Step: 43500, loss: 0.09964, acc: 0.90000\n",
      "[6541 6659 7792 ...  721 8680 6843]\n",
      "[Train] Step: 44000, loss: 0.05001, acc: 0.95000\n",
      "[3102 5067 5980 ... 1355 3973 8446]\n",
      "[Train] Step: 44500, loss: 0.10027, acc: 0.90000\n",
      "[7492 2194 3443 ... 7434 5639 2834]\n",
      "[Train] Step: 45000, loss: 0.30361, acc: 0.70000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 45000, acc: 0.81300\n",
      "[ 497 6514  955 ... 9143 1807 1041]\n",
      "[Train] Step: 45500, loss: 0.10000, acc: 0.90000\n",
      "[9582 3381 9074 ... 5950 7988 1303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 46000, loss: 0.10012, acc: 0.90000\n",
      "[8464 7717 6783 ...  970 1313 7024]\n",
      "[Train] Step: 46500, loss: 0.15515, acc: 0.85000\n",
      "[3689  306 8441 ... 3303 2573 4145]\n",
      "[Train] Step: 47000, loss: 0.16465, acc: 0.80000\n",
      "[ 630 4318 6126 ... 5418 4283 5949]\n",
      "[Train] Step: 47500, loss: 0.10089, acc: 0.90000\n",
      "[3454 4207 8904 ... 6990 7444  856]\n",
      "[Train] Step: 48000, loss: 0.11431, acc: 0.85000\n",
      "[2247 8966 9241 ... 4223 3557 2025]\n",
      "[Train] Step: 48500, loss: 0.09999, acc: 0.90000\n",
      "[9455 6818 7018 ... 7472 2601 6570]\n",
      "[Train] Step: 49000, loss: 0.15034, acc: 0.85000\n",
      "[7114 6339 3033 ... 7265 4529 8689]\n",
      "[Train] Step: 49500, loss: 0.10001, acc: 0.90000\n",
      "[8317 6250 3030 ... 2460 6746 4717]\n",
      "[Train] Step: 50000, loss: 0.21141, acc: 0.80000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 50000, acc: 0.81300\n",
      "[7664 6399 3994 ... 8105 7044 3580]\n",
      "[Train] Step: 50500, loss: 0.10013, acc: 0.90000\n",
      "[6047 8992 7167 ... 1597 7097 3998]\n",
      "[Train] Step: 51000, loss: 0.10179, acc: 0.90000\n",
      "[1253 9431  859 ... 2533 8035 6055]\n",
      "[Train] Step: 51500, loss: 0.11652, acc: 0.85000\n",
      "[4917  961 6628 ... 8917 1697 4094]\n",
      "[Train] Step: 52000, loss: 0.15315, acc: 0.85000\n",
      "[5000   75 8952 ... 5877 3287 1752]\n",
      "[Train] Step: 52500, loss: 0.15275, acc: 0.85000\n",
      "[3208 5617 1842 ... 6350 7110 9234]\n",
      "[Train] Step: 53000, loss: 0.15307, acc: 0.85000\n",
      "[2169 3947 8465 ... 5868  139 4056]\n",
      "[Train] Step: 53500, loss: 0.05000, acc: 0.95000\n",
      "[1689 4827  485 ... 7285 5665 7311]\n",
      "[Train] Step: 54000, loss: 0.25929, acc: 0.75000\n",
      "[3792 6567 2997 ... 6758  222 2931]\n",
      "[Train] Step: 54500, loss: 0.25000, acc: 0.75000\n",
      "[3533 7630 2167 ... 2636 7243 3710]\n",
      "[Train] Step: 55000, loss: 0.04989, acc: 0.95000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 55000, acc: 0.81450\n",
      "[9319 7520 7166 ... 4617 9977 2344]\n",
      "[Train] Step: 55500, loss: 0.05161, acc: 0.95000\n",
      "[3159  815 3230 ...  575 5137 4918]\n",
      "[Train] Step: 56000, loss: 0.15969, acc: 0.85000\n",
      "[5717 3502 8476 ... 9293 2347 4389]\n",
      "[Train] Step: 56500, loss: 0.14936, acc: 0.85000\n",
      "[3582 9943 8157 ... 7478 9633 1334]\n",
      "[Train] Step: 57000, loss: 0.25000, acc: 0.75000\n",
      "[9487 9221 4953 ... 2300 6850  999]\n",
      "[Train] Step: 57500, loss: 0.05125, acc: 0.95000\n",
      "[5585 8234 9948 ... 9980 3587 6584]\n",
      "[Train] Step: 58000, loss: 0.11092, acc: 0.90000\n",
      "[   2 5745 1761 ... 5222  493 2199]\n",
      "[Train] Step: 58500, loss: 0.30667, acc: 0.70000\n",
      "[ 552 4876 9201 ... 4552 1037  774]\n",
      "[Train] Step: 59000, loss: 0.24501, acc: 0.75000\n",
      "[8664 5998 2184 ... 6410 7680 7631]\n",
      "[Train] Step: 59500, loss: 0.25086, acc: 0.75000\n",
      "[2485 2157 7773 ... 4037 5859 7135]\n",
      "[Train] Step: 60000, loss: 0.20000, acc: 0.80000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 60000, acc: 0.81250\n",
      "[7776 4622 6880 ... 6945 9166 3594]\n",
      "[Train] Step: 60500, loss: 0.17593, acc: 0.80000\n",
      "[6145 2778 4188 ... 9142 3764 6348]\n",
      "[Train] Step: 61000, loss: 0.34999, acc: 0.65000\n",
      "[8630 2132 1656 ... 2968 4325 3860]\n",
      "[Train] Step: 61500, loss: 0.05204, acc: 0.95000\n",
      "[7997  853 2577 ... 9230 7538 1156]\n",
      "[Train] Step: 62000, loss: 0.05113, acc: 0.95000\n",
      "[5093 7594 7335 ... 4690 5416 6863]\n",
      "[Train] Step: 62500, loss: 0.04928, acc: 0.95000\n",
      "[1654 6143 5356 ... 2020 5112 5107]\n",
      "[Train] Step: 63000, loss: 0.11245, acc: 0.90000\n",
      "[3360 7799 1363 ... 7534 4467  454]\n",
      "[Train] Step: 63500, loss: 0.19902, acc: 0.80000\n",
      "[ 879 4151  124 ... 8721 8408 6536]\n",
      "[Train] Step: 64000, loss: 0.04793, acc: 0.95000\n",
      "[3054 6087 4904 ... 3807 1178 9940]\n",
      "[Train] Step: 64500, loss: 0.00020, acc: 1.00000\n",
      "[6103 6447 2894 ... 9394 1713 1992]\n",
      "[Train] Step: 65000, loss: 0.10384, acc: 0.90000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 65000, acc: 0.81200\n",
      "[9740 6422 3511 ... 4380 8966 2969]\n",
      "[Train] Step: 65500, loss: 0.05016, acc: 0.95000\n",
      "[1277 4613 5417 ... 7318 1726 2254]\n",
      "[Train] Step: 66000, loss: 0.09809, acc: 0.90000\n",
      "[2169 1356 9932 ... 8377 1840 9136]\n",
      "[Train] Step: 66500, loss: 0.11248, acc: 0.90000\n",
      "[3339 3986 2928 ... 5453 4465 6905]\n",
      "[Train] Step: 67000, loss: 0.10067, acc: 0.90000\n",
      "[7989 7351 6165 ... 3665  222  374]\n",
      "[Train] Step: 67500, loss: 0.10047, acc: 0.90000\n",
      "[9704 2247 4568 ... 4129 4515 7134]\n",
      "[Train] Step: 68000, loss: 0.15000, acc: 0.85000\n",
      "[8864 7718  923 ... 6504 2090 5017]\n",
      "[Train] Step: 68500, loss: 0.24993, acc: 0.75000\n",
      "[3644  590 6370 ...  258 9611 1612]\n",
      "[Train] Step: 69000, loss: 0.31781, acc: 0.65000\n",
      "[3130 6764 6128 ... 9563 6812 8439]\n",
      "[Train] Step: 69500, loss: 0.10023, acc: 0.90000\n",
      "[ 493 1502  465 ... 1359 9707 2622]\n",
      "[Train] Step: 70000, loss: 0.20023, acc: 0.80000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 70000, acc: 0.81150\n",
      "[8661 4520 6941 ... 4018 3063 3458]\n",
      "[Train] Step: 70500, loss: 0.10000, acc: 0.90000\n",
      "[1083 1964 5905 ...  388 7382 6937]\n",
      "[Train] Step: 71000, loss: 0.15040, acc: 0.85000\n",
      "[8877  118 9263 ... 3010 5143 4821]\n",
      "[Train] Step: 71500, loss: 0.10007, acc: 0.90000\n",
      "[ 153 4151 8226 ... 3293 5675 4704]\n",
      "[Train] Step: 72000, loss: 0.19858, acc: 0.80000\n",
      "[1421 7604 6464 ... 8442 7100 2332]\n",
      "[Train] Step: 72500, loss: 0.00000, acc: 1.00000\n",
      "[4319 1575 4695 ... 2747 2056 7312]\n",
      "[Train] Step: 73000, loss: 0.10862, acc: 0.90000\n",
      "[9095 3708  974 ... 6704 2927 2432]\n",
      "[Train] Step: 73500, loss: 0.13096, acc: 0.85000\n",
      "[4418 6165 3968 ... 3600 8864 1141]\n",
      "[Train] Step: 74000, loss: 0.20181, acc: 0.80000\n",
      "[2733 4678 1872 ... 8609 9037 1444]\n",
      "[Train] Step: 74500, loss: 0.10001, acc: 0.90000\n",
      "[3806 8352 8849 ... 2937 8546 6575]\n",
      "[Train] Step: 75000, loss: 0.06472, acc: 0.95000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 75000, acc: 0.81300\n",
      "[5069 9433 7346 ... 9111 1307 7008]\n",
      "[Train] Step: 75500, loss: 0.31578, acc: 0.65000\n",
      "[ 428 2996 5831 ... 9625 6708 1583]\n",
      "[Train] Step: 76000, loss: 0.15044, acc: 0.85000\n",
      "[4133 1973 4364 ...  765  830 9241]\n",
      "[Train] Step: 76500, loss: 0.21117, acc: 0.80000\n",
      "[7524 3178 9854 ... 2705 2051 1660]\n",
      "[Train] Step: 77000, loss: 0.00066, acc: 1.00000\n",
      "[3776 4857 5336 ... 9630 4173 3681]\n",
      "[Train] Step: 77500, loss: 0.05001, acc: 0.95000\n",
      "[2839 5987 6574 ... 5615 1156 5227]\n",
      "[Train] Step: 78000, loss: 0.15900, acc: 0.85000\n",
      "[4218 2519 2661 ...  656 3321  514]\n",
      "[Train] Step: 78500, loss: 0.27470, acc: 0.70000\n",
      "[5507 5489 1627 ... 6030  113 9343]\n",
      "[Train] Step: 79000, loss: 0.25000, acc: 0.75000\n",
      "[1543 8609 2437 ... 9376 5608  742]\n",
      "[Train] Step: 79500, loss: 0.10120, acc: 0.90000\n",
      "[4607  103 5833 ... 2082  283 2180]\n",
      "[Train] Step: 80000, loss: 0.20010, acc: 0.80000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 80000, acc: 0.81400\n",
      "[5438 9883 5208 ...  723 8670 5736]\n",
      "[Train] Step: 80500, loss: 0.10001, acc: 0.90000\n",
      "[3166 2179 1928 ... 8656 3055 5172]\n",
      "[Train] Step: 81000, loss: 0.10162, acc: 0.90000\n",
      "[5116 6606  911 ...   95 2745  511]\n",
      "[Train] Step: 81500, loss: 0.10010, acc: 0.90000\n",
      "[3946  335 4621 ... 3734 8633 1617]\n",
      "[Train] Step: 82000, loss: 0.15084, acc: 0.85000\n",
      "[6398  748 8504 ... 6295 2730 3589]\n",
      "[Train] Step: 82500, loss: 0.05014, acc: 0.95000\n",
      "[1829  797 4792 ... 4192 9705 2306]\n",
      "[Train] Step: 83000, loss: 0.30000, acc: 0.70000\n",
      "[2615 9284 7188 ... 9941 4192 8465]\n",
      "[Train] Step: 83500, loss: 0.11369, acc: 0.85000\n",
      "[5773 7746 4512 ... 4789 2930 3541]\n",
      "[Train] Step: 84000, loss: 0.05018, acc: 0.95000\n",
      "[3016 8767 3891 ... 4509 5234 2048]\n",
      "[Train] Step: 84500, loss: 0.09998, acc: 0.90000\n",
      "[3305 2524 3104 ... 6896 7077  495]\n",
      "[Train] Step: 85000, loss: 0.25709, acc: 0.75000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 85000, acc: 0.81400\n",
      "[3582 9383 2703 ... 5211 2968 6280]\n",
      "[Train] Step: 85500, loss: 0.14999, acc: 0.85000\n",
      "[8313 8113 7062 ...  572 3872 3338]\n",
      "[Train] Step: 86000, loss: 0.10243, acc: 0.90000\n",
      "[5170 2276 1867 ... 5686 3644 8764]\n",
      "[Train] Step: 86500, loss: 0.04995, acc: 0.95000\n",
      "[6138 5626 1772 ... 4474 5752 8800]\n",
      "[Train] Step: 87000, loss: 0.10493, acc: 0.90000\n",
      "[9694 7700 6848 ...    0 1712 2868]\n",
      "[Train] Step: 87500, loss: 0.10001, acc: 0.90000\n",
      "[5677 1709 9398 ... 6846 1692 2785]\n",
      "[Train] Step: 88000, loss: 0.11527, acc: 0.85000\n",
      "[2654 5165 5413 ... 1515 1797 7774]\n",
      "[Train] Step: 88500, loss: 0.10764, acc: 0.90000\n",
      "[1174 8569 9942 ... 6516 2225 4888]\n",
      "[Train] Step: 89000, loss: 0.10054, acc: 0.90000\n",
      "[1847 6082 2450 ... 7407 6135 4247]\n",
      "[Train] Step: 89500, loss: 0.25741, acc: 0.75000\n",
      "[6496 1079 1765 ...   49 2893 1178]\n",
      "[Train] Step: 90000, loss: 0.35004, acc: 0.65000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 90000, acc: 0.81400\n",
      "[1697 2746 3672 ... 8846 3437 5684]\n",
      "[Train] Step: 90500, loss: 0.05197, acc: 0.95000\n",
      "[3677 1964 8943 ... 4482 9134 1863]\n",
      "[Train] Step: 91000, loss: 0.25027, acc: 0.75000\n",
      "[9746 1709 2988 ... 6678 1471 7743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 91500, loss: 0.10003, acc: 0.90000\n",
      "[4758 2819 7476 ... 8985 3188 4581]\n",
      "[Train] Step: 92000, loss: 0.21662, acc: 0.75000\n",
      "[2145 5279  191 ... 5105 6602  688]\n",
      "[Train] Step: 92500, loss: 0.14955, acc: 0.85000\n",
      "[9481 4664 9468 ... 3061 5429 2049]\n",
      "[Train] Step: 93000, loss: 0.11313, acc: 0.90000\n",
      "[1592 9637 5687 ... 1762 2785 9987]\n",
      "[Train] Step: 93500, loss: 0.05375, acc: 0.95000\n",
      "[7093 1679 1215 ... 5058 3325 1847]\n",
      "[Train] Step: 94000, loss: 0.10000, acc: 0.90000\n",
      "[5844 3381 4772 ... 5916  697 7259]\n",
      "[Train] Step: 94500, loss: 0.15176, acc: 0.85000\n",
      "[2053 8785 7896 ... 4728 3972 9140]\n",
      "[Train] Step: 95000, loss: 0.10018, acc: 0.90000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 95000, acc: 0.81200\n",
      "[3824 2828 5226 ...  626 2877 3368]\n",
      "[Train] Step: 95500, loss: 0.10031, acc: 0.90000\n",
      "[3040  806 9414 ... 4001 3831 3733]\n",
      "[Train] Step: 96000, loss: 0.15015, acc: 0.85000\n",
      "[3928 4247 5595 ... 9924 3217 7278]\n",
      "[Train] Step: 96500, loss: 0.10967, acc: 0.90000\n",
      "[1688 4303 5394 ... 6017 9420 5112]\n",
      "[Train] Step: 97000, loss: 0.05425, acc: 0.95000\n",
      "[8408 7347 5165 ...  992  732 8344]\n",
      "[Train] Step: 97500, loss: 0.10132, acc: 0.90000\n",
      "[2432 2389 7684 ... 9040 8501 4808]\n",
      "[Train] Step: 98000, loss: 0.19382, acc: 0.80000\n",
      "[2673 8653 4671 ...   40 9627  849]\n",
      "[Train] Step: 98500, loss: 0.05466, acc: 0.95000\n",
      "[8055 6960 1793 ... 3527 5743 8126]\n",
      "[Train] Step: 99000, loss: 0.10369, acc: 0.90000\n",
      "[ 442 2435 2730 ... 4875 4110 9381]\n",
      "[Train] Step: 99500, loss: 0.10967, acc: 0.90000\n",
      "[1002 8176 9780 ... 6231  915  701]\n",
      "[Train] Step: 100000, loss: 0.10339, acc: 0.90000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Step: 100000, acc: 0.81500\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "batch_size = 20\n",
    "train_steps = 100000\n",
    "test_steps = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(train_steps):\n",
    "        batch_data, batch_labels = train_data.next_batch(batch_size)\n",
    "        loss_val, acc_val, _ = sess.run(\n",
    "            [loss, accuracy, train_op],\n",
    "            feed_dict = {\n",
    "                x: batch_data,\n",
    "                y: batch_labels\n",
    "            }\n",
    "        )\n",
    "        if (i+1) % 500 == 0:\n",
    "            print('[Train] Step: %d, loss: %4.5f, acc: %4.5f' \\\n",
    "                % (i+1, loss_val, acc_val))\n",
    "        if (i+1) % 5000 == 0:\n",
    "            test_data = CifarData(test_filenames, False)\n",
    "            all_test_acc_val = []\n",
    "            for j in range(test_steps):\n",
    "                test_batch_data, test_batch_labels \\\n",
    "                    = test_data.next_batch(batch_size) \n",
    "                test_acc_val = sess.run(\n",
    "                    [accuracy],\n",
    "                    feed_dict = {\n",
    "                        x: test_batch_data,\n",
    "                        y: test_batch_labels\n",
    "                    }\n",
    "                )\n",
    "                all_test_acc_val.append(test_acc_val)\n",
    "            test_acc = np.mean(all_test_acc_val)\n",
    "            print('[Test] Step: %d, acc: %4.5f' % (i+1, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
