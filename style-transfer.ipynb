{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "class VGGNet:\n",
    "    \n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "        \n",
    "    def get_conv_fiter(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='conv')\n",
    "    \n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='fc')\n",
    "    \n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name='bias')\n",
    "    \n",
    "    def conv_layer(self, x, name):\n",
    "        with tf.name_scope(name):\n",
    "            conv_w = self.get_conv_fiter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            h = tf.nn.conv2d(x, conv_w, [1,1,1,1], padding='SAME')\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "        \n",
    "    def pooling_layer(self, x, name):\n",
    "        \"\"\"builds pooling layer\"\"\"\n",
    "        return tf.nn.max_pool(x, \n",
    "                             ksize = [1,2,2,1],\n",
    "                             strides = [1,2,2,1],\n",
    "                             padding = 'SAME',\n",
    "                             name = name )\n",
    "    \n",
    "    def fc_layer(self, x, name, activation=tf.nn.relu):\n",
    "        \"\"\"build fully-connected layer\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(x, fc_w)\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            if activation == None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "        \n",
    "    def flatten_layer(self, x, name):\n",
    "        \"\"\"build flatten layer\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(x, [-1, dim])\n",
    "            return x\n",
    "        \n",
    "    def build(self, x_rgb):\n",
    "        \"\"\"build VGG16 network structure.\n",
    "        parameter: \n",
    "        - x_rgb: [1, 244, 244, 3]\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print('building model....')\n",
    "        \n",
    "        r, g, b = tf.split(x_rgb, [1,1,1], axis=3) # axis表示上面的x_rgb里的3, 就是将这个三通道分离出来，分离成r, g, b\n",
    "        x_bgr = tf.concat(                       #  减去均值\n",
    "                        [b - VGG_MEAN[0],\n",
    "                        g - VGG_MEAN[1],\n",
    "                        r - VGG_MEAN[2]],\n",
    "                        axis = 3 )\n",
    "        \n",
    "        assert x_bgr.get_shape().as_list()[1:] == [224,224,3]\n",
    "        \n",
    "        self.conv1_1 = self.conv_layer(x_bgr, b'conv1_1') # 这里的名字必须跟pre model也就是vgg16里的命名一致\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, b'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1')\n",
    "        \n",
    "        self.conv2_1 = self.conv_layer(self.pool1, b'conv2_1') \n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, b'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "        \n",
    "        self.conv3_1 = self.conv_layer(self.pool2, b'conv3_1') \n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, b'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, b'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "        \n",
    "        self.conv4_1 = self.conv_layer(self.pool3, b'conv4_1') \n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, b'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, b'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "        \n",
    "        self.conv5_1 = self.conv_layer(self.pool4, b'conv5_1') \n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, b'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, b'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "        \n",
    "        # 这里暂时注释，耗时最多的在构建全连接层\n",
    "#         self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "#         self.fc6 = self.fc_layer(self.flatten5, b'fc6')\n",
    "#         self.fc7 = self.fc_layer(self.fc6, b'fc7')\n",
    "#         self.fc8 = self.fc_layer(self.fc7, b'fc8', activation=None)\n",
    "#         self.prob = tf.nn.softmax(self.fc8, name='prob')\n",
    "        \n",
    "        print('building modle finished: %4ds' % (time.time() - start_time))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试计算图\n",
    "# vgg16_npy_path = './vgg16.npy'\n",
    "# data_dict = np.load(file=vgg16_npy_path, encoding='bytes').item()\n",
    "\n",
    "# vgg16_for_result = VGGNet(data_dict)\n",
    "# content = tf.placeholder(tf.float32, shape=[1,224,224,3])\n",
    "# vgg16_for_result.build(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vgg16_npy_path = './vgg16.npy'\n",
    "content_img_path = './source_images/gugong.jpg'\n",
    "style_img_path = './source_images/xingkong.jpeg'\n",
    "\n",
    "num_steps = 100\n",
    "learning_rate = 10\n",
    "\n",
    "lambda_c = 0.1\n",
    "lambda_s = 500\n",
    "\n",
    "output_dir = './style_transfer'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "building model....\n",
      "building modle finished:    0s\n",
      "building model....\n",
      "building modle finished:    0s\n",
      "building model....\n",
      "building modle finished:    0s\n",
      "WARNING:tensorflow:From c:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "def initial_result(shape, mean, stddev):\n",
    "    initial = tf.truncated_normal(shape, mean = mean, stddev = stddev)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def read_img(img_name):\n",
    "    img = Image.open(img_name)\n",
    "    np_img = np.array(img) # (224,224,3)\n",
    "    np_img = np.asarray([np_img], dtype=np.int32) # (1, 224, 224, 3)\n",
    "    return np_img\n",
    "\n",
    "def gram_matrix(x):\n",
    "    \"\"\"Calculate gram matrix\n",
    "    Args:\n",
    "    -x: features extracted from vgg net. shape: [1, width, height, chanel]\n",
    "    \"\"\"\n",
    "    b, w, h, ch = x.get_shape().as_list()\n",
    "    features = tf.reshape(x, [b, h*w, ch]) # [ch, ch] -> (i, j)\n",
    "    \n",
    "    gram = tf.matmul(features, features, adjoint_a=True) \\\n",
    "        / tf.constant(ch * w * h, tf.float32)\n",
    "    return gram\n",
    "\n",
    "result = initial_result((1,224,224,3), 127.5, 20)\n",
    "\n",
    "content_val = read_img(content_img_path)\n",
    "style_val = read_img(style_img_path)\n",
    "\n",
    "content = tf.placeholder(tf.float32, shape=[1,224,224,3])\n",
    "style = tf.placeholder(tf.float32, shape=[1,224,224,3])    \n",
    "    \n",
    "data_dict = np.load(file=vgg16_npy_path, encoding='bytes').item()\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "\n",
    "\n",
    "content_features = [\n",
    "    vgg_for_content.conv1_2,\n",
    "    vgg_for_content.conv2_2,\n",
    "#     vgg_for_content.conv3_3,\n",
    "#     vgg_for_content.conv4_3,\n",
    "#     vgg_for_content.conv5_3\n",
    "]\n",
    "\n",
    "result_content_features = [\n",
    "    vgg_for_result.conv1_2,\n",
    "    vgg_for_result.conv2_2,\n",
    "#     vgg_for_result.conv3_3,\n",
    "#     vgg_for_result.conv4_3,\n",
    "#     vgg_for_result.conv5_3\n",
    "]\n",
    "\n",
    "style_features = [\n",
    "#     vgg_for_style.conv1_2,\n",
    "#     vgg_for_style.conv2_2,\n",
    "#     vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3,\n",
    "#     vgg_for_style.conv5_3,\n",
    "]\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "result_style_features = [\n",
    "#     vgg_for_result.conv1_2,\n",
    "#     vgg_for_result.conv2_2,\n",
    "#     vgg_for_result.conv3_3,\n",
    "    vgg_for_result.conv4_3,\n",
    "#     vgg_for_result.conv5_3,\n",
    "]\n",
    "\n",
    "result_style_gram = [gram_matrix(feature) for feature in result_style_features]\n",
    "\n",
    "content_loss = tf.zeros(1, tf.float32)\n",
    "# zip: [1,2],[3,4], zip([1,2],[3,4]) -> [(1,3),(2,4)]\n",
    "# shape: [1, width, height, channel]\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1,2,3]) # [1,2,3] 表示width, height, channel这三项\n",
    "    \n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1,2])\n",
    "    \n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, loss_value: 34397.1875, style_loss: 263016.5000\n",
      "Step 2, loss_value: 27787.4961, style_loss: 202175.2969\n",
      "Step 3, loss_value: 22525.6016, style_loss: 163661.5625\n",
      "Step 4, loss_value: 18768.7266, style_loss: 141249.2812\n",
      "Step 5, loss_value: 16539.6777, style_loss: 127344.6719\n",
      "Step 6, loss_value: 15376.5205, style_loss: 118434.9609\n",
      "Step 7, loss_value: 14488.3857, style_loss: 111975.1797\n",
      "Step 8, loss_value: 13333.8027, style_loss: 106350.8984\n",
      "Step 9, loss_value: 12514.1963, style_loss: 100315.5469\n",
      "Step 10, loss_value: 11652.3887, style_loss: 93296.4219\n",
      "Step 11, loss_value: 10760.5742, style_loss: 86291.7812\n",
      "Step 12, loss_value: 9980.4053, style_loss: 79389.1953\n",
      "Step 13, loss_value: 9301.1807, style_loss: 72616.1641\n",
      "Step 14, loss_value: 8625.6221, style_loss: 66653.3203\n",
      "Step 15, loss_value: 8121.3413, style_loss: 61448.7812\n",
      "Step 16, loss_value: 7623.6665, style_loss: 57172.3477\n",
      "Step 17, loss_value: 7248.0312, style_loss: 53338.1211\n",
      "Step 18, loss_value: 6913.1982, style_loss: 50324.4883\n",
      "Step 19, loss_value: 6913.1553, style_loss: 47725.2344\n",
      "Step 20, loss_value: 6473.3511, style_loss: 46296.7109\n",
      "Step 21, loss_value: 6244.4653, style_loss: 44826.4492\n",
      "Step 22, loss_value: 6032.9863, style_loss: 43260.7031\n",
      "Step 23, loss_value: 5830.5200, style_loss: 41507.2578\n",
      "Step 24, loss_value: 5782.1172, style_loss: 39433.1094\n",
      "Step 25, loss_value: 5702.6660, style_loss: 38299.2266\n",
      "Step 26, loss_value: 5696.1045, style_loss: 37246.8672\n",
      "Step 27, loss_value: 5374.4004, style_loss: 36901.8438\n",
      "Step 28, loss_value: 5299.7168, style_loss: 36468.3359\n",
      "Step 29, loss_value: 5252.7148, style_loss: 35469.8477\n",
      "Step 30, loss_value: 5086.9062, style_loss: 34542.1328\n",
      "Step 31, loss_value: 4980.3760, style_loss: 33008.4844\n",
      "Step 32, loss_value: 4775.1357, style_loss: 31881.6992\n",
      "Step 33, loss_value: 4718.4863, style_loss: 30777.7129\n",
      "Step 34, loss_value: 4625.5303, style_loss: 30110.2461\n",
      "Step 35, loss_value: 4699.2583, style_loss: 29274.0977\n",
      "Step 36, loss_value: 4645.0063, style_loss: 29117.2559\n",
      "Step 37, loss_value: 4587.9141, style_loss: 28236.4980\n",
      "Step 38, loss_value: 4373.2944, style_loss: 27673.0156\n",
      "Step 39, loss_value: 4353.3867, style_loss: 27347.5488\n",
      "Step 40, loss_value: 4312.3081, style_loss: 26844.7598\n",
      "Step 41, loss_value: 4256.9170, style_loss: 26606.7676\n",
      "Step 42, loss_value: 4350.6392, style_loss: 26085.6250\n",
      "Step 43, loss_value: 4192.2534, style_loss: 26218.8203\n",
      "Step 44, loss_value: 4156.4648, style_loss: 25915.8477\n",
      "Step 45, loss_value: 4059.0928, style_loss: 25425.6094\n",
      "Step 46, loss_value: 3987.9966, style_loss: 24636.0566\n",
      "Step 47, loss_value: 3915.8545, style_loss: 23967.3594\n",
      "Step 48, loss_value: 3901.9956, style_loss: 23300.3242\n",
      "Step 49, loss_value: 3908.1443, style_loss: 23055.4004\n",
      "Step 50, loss_value: 4211.4956, style_loss: 22511.3203\n",
      "Step 51, loss_value: 4000.2437, style_loss: 23073.2422\n",
      "Step 52, loss_value: 3940.9478, style_loss: 23475.3672\n",
      "Step 53, loss_value: 3958.8696, style_loss: 23773.5312\n",
      "Step 54, loss_value: 3895.5957, style_loss: 23689.2852\n",
      "Step 55, loss_value: 3885.1025, style_loss: 23200.5195\n",
      "Step 56, loss_value: 3835.4175, style_loss: 23021.4941\n",
      "Step 57, loss_value: 3861.4651, style_loss: 22575.4375\n",
      "Step 58, loss_value: 3794.9915, style_loss: 22536.1504\n",
      "Step 59, loss_value: 3772.9302, style_loss: 22056.7891\n",
      "Step 60, loss_value: 3674.7209, style_loss: 21779.5410\n",
      "Step 61, loss_value: 3675.5200, style_loss: 21315.1191\n",
      "Step 62, loss_value: 3626.4556, style_loss: 21130.9375\n",
      "Step 63, loss_value: 3682.5884, style_loss: 20757.3906\n",
      "Step 64, loss_value: 3683.0703, style_loss: 20991.9043\n",
      "Step 65, loss_value: 3734.9478, style_loss: 20633.5312\n",
      "Step 66, loss_value: 3582.3872, style_loss: 20624.6016\n",
      "Step 67, loss_value: 3593.1687, style_loss: 20463.7402\n",
      "Step 68, loss_value: 3532.5459, style_loss: 20497.0215\n",
      "Step 69, loss_value: 3488.3325, style_loss: 20317.8555\n",
      "Step 70, loss_value: 3457.5295, style_loss: 19960.4355\n",
      "Step 71, loss_value: 3412.2769, style_loss: 19536.4551\n",
      "Step 72, loss_value: 3378.4111, style_loss: 19296.0625\n",
      "Step 73, loss_value: 3402.7505, style_loss: 18929.3301\n",
      "Step 74, loss_value: 3563.0874, style_loss: 19048.4629\n",
      "Step 75, loss_value: 3977.9739, style_loss: 19113.7012\n",
      "Step 76, loss_value: 3733.9438, style_loss: 20963.3438\n",
      "Step 77, loss_value: 3765.2710, style_loss: 22185.6328\n",
      "Step 78, loss_value: 3786.8003, style_loss: 22446.3750\n",
      "Step 79, loss_value: 3701.7771, style_loss: 22183.1758\n",
      "Step 80, loss_value: 3681.5059, style_loss: 21495.5156\n",
      "Step 81, loss_value: 3640.4795, style_loss: 21364.4531\n",
      "Step 82, loss_value: 3647.7085, style_loss: 21000.2969\n",
      "Step 83, loss_value: 3563.3745, style_loss: 20867.7109\n",
      "Step 84, loss_value: 3520.5747, style_loss: 20272.8281\n",
      "Step 85, loss_value: 3443.5193, style_loss: 19865.9453\n",
      "Step 86, loss_value: 3461.0186, style_loss: 19319.3379\n",
      "Step 87, loss_value: 3517.6167, style_loss: 19351.6094\n",
      "Step 88, loss_value: 3849.6609, style_loss: 19289.5723\n",
      "Step 89, loss_value: 3742.4390, style_loss: 20466.2246\n",
      "Step 90, loss_value: 3704.1826, style_loss: 21169.3652\n",
      "Step 91, loss_value: 3675.4233, style_loss: 21603.0117\n",
      "Step 92, loss_value: 3647.8599, style_loss: 21560.8203\n",
      "Step 93, loss_value: 3614.1030, style_loss: 21036.0781\n",
      "Step 94, loss_value: 3566.9272, style_loss: 20657.0703\n",
      "Step 95, loss_value: 3585.9126, style_loss: 20216.1758\n",
      "Step 96, loss_value: 3500.9917, style_loss: 20318.1641\n",
      "Step 97, loss_value: 3532.4585, style_loss: 19895.3652\n",
      "Step 98, loss_value: 3473.1907, style_loss: 19648.1953\n",
      "Step 99, loss_value: 3517.1431, style_loss: 19240.1348\n",
      "Step 100, loss_value: 3428.1338, style_loss: 19302.9043\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        loss_value, content_loss_value, style_loss_value, _ \\\n",
    "            = sess.run([loss, content_loss, style_loss, train_op], \n",
    "                      feed_dict={\n",
    "                          content: content_val,\n",
    "                          style: style_val\n",
    "                      })\n",
    "        print('Step %d, loss_value: %8.4f, style_loss: %8.4f' % (step+1, loss_value[0], content_loss_value[0]))\n",
    "        \n",
    "        result_img_path = os.path.join(output_dir, 'result_%2d.jpg' % (step+1))\n",
    "        result_val = result.eval(sess)[0]\n",
    "        result_val = np.clip(result_val, 0, 255)\n",
    "        img_arr = np.asarray(result_val, np.uint8)\n",
    "        img = Image.fromarray(img_arr)\n",
    "        img.save(result_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
