{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "class VGGNet:\n",
    "    \n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "        \n",
    "    def get_conv_fiter(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='conv')\n",
    "    \n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='fc')\n",
    "    \n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name='bias')\n",
    "    \n",
    "    def conv_layer(self, x, name):\n",
    "        with tf.name_scope(name):\n",
    "            conv_w = self.get_conv_fiter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            h = tf.nn.conv2d(x, conv_w, [1,1,1,1], padding='SAME')\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "        \n",
    "    def pooling_layer(self, x, name):\n",
    "        \"\"\"builds pooling layer\"\"\"\n",
    "        return tf.nn.max_pool(x, \n",
    "                             ksize = [1,2,2,1],\n",
    "                             strides = [1,2,2,1],\n",
    "                             padding = 'SAME',\n",
    "                             name = name )\n",
    "    \n",
    "    def fc_layer(self, x, name, activation=tf.nn.relu):\n",
    "        \"\"\"build fully-connected layer\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(x, fc_w)\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            if activation == None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "        \n",
    "    def flatten_layer(self, x, name):\n",
    "        \"\"\"build flatten layer\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(x, [-1, dim])\n",
    "            return x\n",
    "        \n",
    "    def build(self, x_rgb):\n",
    "        \"\"\"build VGG16 network structure.\n",
    "        parameter: \n",
    "        - x_rgb: [1, 244, 244, 3]\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print('building model....')\n",
    "        \n",
    "        r, g, b = tf.split(x_rgb, [1,1,1], axis=3) # axis表示上面的x_rgb里的3, 就是将这个三通道分离出来，分离成r, g, b\n",
    "        x_bgr = tf.concat(                       #  减去均值\n",
    "                        [b - VGG_MEAN[0],\n",
    "                        g - VGG_MEAN[1],\n",
    "                        r - VGG_MEAN[2]],\n",
    "                        axis = 3 )\n",
    "        \n",
    "        assert x_bgr.get_shape().as_list()[1:] == [224,224,3]\n",
    "        \n",
    "        self.conv1_1 = self.conv_layer(x_bgr, b'conv1_1') # 这里的名字必须跟pre model也就是vgg16里的命名一致\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, b'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1')\n",
    "        \n",
    "        self.conv2_1 = self.conv_layer(self.pool1, b'conv2_1') \n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, b'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "        \n",
    "        self.conv3_1 = self.conv_layer(self.pool2, b'conv3_1') \n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, b'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, b'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "        \n",
    "        self.conv4_1 = self.conv_layer(self.pool3, b'conv4_1') \n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, b'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, b'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "        \n",
    "        self.conv5_1 = self.conv_layer(self.pool4, b'conv5_1') \n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, b'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, b'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "        \n",
    "        # 这里暂时注释，耗时最多的在构建全连接层\n",
    "#         self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "#         self.fc6 = self.fc_layer(self.flatten5, b'fc6')\n",
    "#         self.fc7 = self.fc_layer(self.fc6, b'fc7')\n",
    "#         self.fc8 = self.fc_layer(self.fc7, b'fc8', activation=None)\n",
    "#         self.prob = tf.nn.softmax(self.fc8, name='prob')\n",
    "        \n",
    "        print('building modle finished: %4ds' % (time.time() - start_time))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试计算图\n",
    "# vgg16_npy_path = './vgg16.npy'\n",
    "# data_dict = np.load(file=vgg16_npy_path, encoding='bytes').item()\n",
    "\n",
    "# vgg16_for_result = VGGNet(data_dict)\n",
    "# content = tf.placeholder(tf.float32, shape=[1,224,224,3])\n",
    "# vgg16_for_result.build(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vgg16_npy_path = './vgg16.npy'\n",
    "content_img_path = './source_images/gugong.jpg'\n",
    "style_img_path = './source_images/xingkong.jpeg'\n",
    "\n",
    "num_steps = 100\n",
    "learning_rate = 10\n",
    "\n",
    "lambda_c = 0.1\n",
    "lambda_s = 500\n",
    "\n",
    "output_dir = './style_transfer'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "building model....\n",
      "building modle finished:    0s\n",
      "building model....\n",
      "building modle finished:    0s\n",
      "building model....\n",
      "building modle finished:    0s\n",
      "WARNING:tensorflow:From c:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "def initial_result(shape, mean, stddev):\n",
    "    initial = tf.truncated_normal(shape, mean = mean, stddev = stddev)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def read_img(img_name):\n",
    "    img = Image.open(img_name)\n",
    "    np_img = np.array(img) # (224,224,3)\n",
    "    np_img = np.asarray([np_img], dtype=np.int32) # (1, 224, 224, 3)\n",
    "    return np_img\n",
    "\n",
    "def gram_matrix(x):\n",
    "    \"\"\"Calculate gram matrix\n",
    "    Args:\n",
    "    -x: features extracted from vgg net. shape: [1, width, height, chanel]\n",
    "    \"\"\"\n",
    "    b, w, h, ch = x.get_shape().as_list()\n",
    "    features = tf.reshape(x, [b, h*w, ch]) # [ch, ch] -> (i, j)\n",
    "    \n",
    "    gram = tf.matmul(features, features, adjoint_a=True) \\\n",
    "        / tf.constant(ch * w * h, tf.float32)\n",
    "    return gram\n",
    "\n",
    "result = initial_result((1,224,224,3), 127.5, 20)\n",
    "\n",
    "content_val = read_img(content_img_path)\n",
    "style_val = read_img(style_img_path)\n",
    "\n",
    "content = tf.placeholder(tf.float32, shape=[1,224,224,3])\n",
    "style = tf.placeholder(tf.float32, shape=[1,224,224,3])    \n",
    "    \n",
    "data_dict = np.load(file=vgg16_npy_path, encoding='bytes').item()\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "\n",
    "\n",
    "content_features = [\n",
    "    vgg_for_content.conv1_2,\n",
    "    vgg_for_content.conv2_2,\n",
    "#     vgg_for_content.conv3_3,\n",
    "#     vgg_for_content.conv4_3,\n",
    "#     vgg_for_content.conv5_3\n",
    "]\n",
    "\n",
    "result_content_features = [\n",
    "    vgg_for_result.conv1_2,\n",
    "    vgg_for_result.conv2_2,\n",
    "#     vgg_for_result.conv3_3,\n",
    "#     vgg_for_result.conv4_3,\n",
    "#     vgg_for_result.conv5_3\n",
    "]\n",
    "\n",
    "style_features = [\n",
    "#     vgg_for_style.conv1_2,\n",
    "#     vgg_for_style.conv2_2,\n",
    "#     vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3,\n",
    "#     vgg_for_style.conv5_3,\n",
    "]\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "result_style_features = [\n",
    "#     vgg_for_result.conv1_2,\n",
    "#     vgg_for_result.conv2_2,\n",
    "#     vgg_for_result.conv3_3,\n",
    "    vgg_for_result.conv4_3,\n",
    "#     vgg_for_result.conv5_3,\n",
    "]\n",
    "\n",
    "result_style_gram = [gram_matrix(feature) for feature in result_style_features]\n",
    "\n",
    "content_loss = tf.zeros(1, tf.float32)\n",
    "# zip: [1,2],[3,4], zip([1,2],[3,4]) -> [(1,3),(2,4)]\n",
    "# shape: [1, width, height, channel]\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1,2,3]) # [1,2,3] 表示width, height, channel这三项\n",
    "    \n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1,2])\n",
    "    \n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, loss_value: 79824264.0000, style_loss: 263954.0312\n",
      "Step 2, loss_value: 46149560.0000, style_loss: 293939.0938\n",
      "Step 3, loss_value: 40006204.0000, style_loss: 341689.2812\n",
      "Step 4, loss_value: 25055864.0000, style_loss: 353238.1250\n",
      "Step 5, loss_value: 18760856.0000, style_loss: 369495.5000\n",
      "Step 6, loss_value: 13066320.0000, style_loss: 392940.3125\n",
      "Step 7, loss_value: 13418598.0000, style_loss: 417563.6250\n",
      "Step 8, loss_value: 11808703.0000, style_loss: 433285.7188\n",
      "Step 9, loss_value: 9922748.0000, style_loss: 444301.5938\n",
      "Step 10, loss_value: 9272264.0000, style_loss: 456036.9375\n",
      "Step 11, loss_value: 8393854.0000, style_loss: 469229.4375\n",
      "Step 12, loss_value: 7636748.0000, style_loss: 481572.4062\n",
      "Step 13, loss_value: 6999128.0000, style_loss: 489816.5625\n",
      "Step 14, loss_value: 6025517.5000, style_loss: 493490.8438\n",
      "Step 15, loss_value: 5366958.5000, style_loss: 495889.6250\n",
      "Step 16, loss_value: 5079812.0000, style_loss: 500106.4062\n",
      "Step 17, loss_value: 4781475.0000, style_loss: 506791.5625\n",
      "Step 18, loss_value: 4574835.5000, style_loss: 514345.0625\n",
      "Step 19, loss_value: 4423208.0000, style_loss: 520080.9688\n",
      "Step 20, loss_value: 4084382.0000, style_loss: 522539.8125\n",
      "Step 21, loss_value: 3753714.5000, style_loss: 522836.1250\n",
      "Step 22, loss_value: 3517059.5000, style_loss: 523070.5312\n",
      "Step 23, loss_value: 3275474.2500, style_loss: 524364.3750\n",
      "Step 24, loss_value: 3099676.2500, style_loss: 526316.3125\n",
      "Step 25, loss_value: 2942504.2500, style_loss: 527640.0000\n",
      "Step 26, loss_value: 2737236.2500, style_loss: 527684.1250\n",
      "Step 27, loss_value: 2589945.2500, style_loss: 527087.3750\n",
      "Step 28, loss_value: 2444673.5000, style_loss: 526765.8125\n",
      "Step 29, loss_value: 2279395.5000, style_loss: 526961.7500\n",
      "Step 30, loss_value: 2169920.0000, style_loss: 527164.7500\n",
      "Step 31, loss_value: 2061869.3750, style_loss: 526765.8125\n",
      "Step 32, loss_value: 1946582.8750, style_loss: 525942.5000\n",
      "Step 33, loss_value: 1857856.5000, style_loss: 525448.1250\n",
      "Step 34, loss_value: 1763617.1250, style_loss: 525545.3750\n",
      "Step 35, loss_value: 1688301.6250, style_loss: 525623.0000\n",
      "Step 36, loss_value: 1607926.3750, style_loss: 524860.1250\n",
      "Step 37, loss_value: 1523417.3750, style_loss: 523311.5000\n",
      "Step 38, loss_value: 1468331.2500, style_loss: 521885.0938\n",
      "Step 39, loss_value: 1402096.7500, style_loss: 521275.4375\n",
      "Step 40, loss_value: 1345448.1250, style_loss: 521265.5312\n",
      "Step 41, loss_value: 1300085.8750, style_loss: 521080.3125\n",
      "Step 42, loss_value: 1247759.6250, style_loss: 520357.6875\n",
      "Step 43, loss_value: 1209154.5000, style_loss: 519515.8125\n",
      "Step 44, loss_value: 1164205.8750, style_loss: 518966.9375\n",
      "Step 45, loss_value: 1124621.0000, style_loss: 518534.9375\n",
      "Step 46, loss_value: 1087212.2500, style_loss: 517816.8750\n",
      "Step 47, loss_value: 1050938.5000, style_loss: 516879.7500\n",
      "Step 48, loss_value: 1019300.2500, style_loss: 516183.5625\n",
      "Step 49, loss_value: 985620.0625, style_loss: 515848.0938\n",
      "Step 50, loss_value: 958799.2500, style_loss: 515468.6875\n",
      "Step 51, loss_value: 928720.8750, style_loss: 514762.5000\n",
      "Step 52, loss_value: 904020.8125, style_loss: 514026.0000\n",
      "Step 53, loss_value: 877987.4375, style_loss: 513615.2188\n",
      "Step 54, loss_value: 855278.9375, style_loss: 513366.4062\n",
      "Step 55, loss_value: 834051.5000, style_loss: 512855.2812\n",
      "Step 56, loss_value: 812053.1875, style_loss: 512105.6250\n",
      "Step 57, loss_value: 793662.3125, style_loss: 511527.2188\n",
      "Step 58, loss_value: 774554.2500, style_loss: 511226.0312\n",
      "Step 59, loss_value: 757170.6875, style_loss: 510866.4375\n",
      "Step 60, loss_value: 739641.9375, style_loss: 510266.3750\n",
      "Step 61, loss_value: 724153.1875, style_loss: 509680.8750\n",
      "Step 62, loss_value: 708243.4375, style_loss: 509305.3438\n",
      "Step 63, loss_value: 694533.0000, style_loss: 508965.4688\n",
      "Step 64, loss_value: 679907.5625, style_loss: 508483.2188\n",
      "Step 65, loss_value: 667475.6875, style_loss: 507996.8438\n",
      "Step 66, loss_value: 654464.9375, style_loss: 507631.4375\n",
      "Step 67, loss_value: 642713.1875, style_loss: 507224.1875\n",
      "Step 68, loss_value: 630880.0625, style_loss: 506658.2188\n",
      "Step 69, loss_value: 620169.8125, style_loss: 506146.8125\n",
      "Step 70, loss_value: 609485.2500, style_loss: 505865.9375\n",
      "Step 71, loss_value: 599613.5625, style_loss: 505627.4375\n",
      "Step 72, loss_value: 589726.7500, style_loss: 505225.5625\n",
      "Step 73, loss_value: 580564.0000, style_loss: 504778.8438\n",
      "Step 74, loss_value: 571378.9375, style_loss: 504441.0625\n",
      "Step 75, loss_value: 562940.6875, style_loss: 504117.3125\n",
      "Step 76, loss_value: 554320.3125, style_loss: 503729.1250\n",
      "Step 77, loss_value: 546422.0625, style_loss: 503389.0000\n",
      "Step 78, loss_value: 538570.9375, style_loss: 503124.6250\n",
      "Step 79, loss_value: 531169.0625, style_loss: 502791.8125\n",
      "Step 80, loss_value: 523909.6250, style_loss: 502383.5625\n",
      "Step 81, loss_value: 516915.5938, style_loss: 502061.9688\n",
      "Step 82, loss_value: 510240.6562, style_loss: 501831.5000\n",
      "Step 83, loss_value: 503675.2500, style_loss: 501547.6250\n",
      "Step 84, loss_value: 497407.6250, style_loss: 501230.2188\n",
      "Step 85, loss_value: 491254.3750, style_loss: 500989.9375\n",
      "Step 86, loss_value: 485389.1562, style_loss: 500767.4688\n",
      "Step 87, loss_value: 479550.3750, style_loss: 500474.1250\n",
      "Step 88, loss_value: 473996.8125, style_loss: 500185.2188\n",
      "Step 89, loss_value: 468497.7500, style_loss: 499962.6875\n",
      "Step 90, loss_value: 463203.0938, style_loss: 499728.0625\n",
      "Step 91, loss_value: 458046.1875, style_loss: 499468.9375\n",
      "Step 92, loss_value: 452993.5000, style_loss: 499266.0938\n",
      "Step 93, loss_value: 448128.5625, style_loss: 499076.7812\n",
      "Step 94, loss_value: 443322.2812, style_loss: 498819.4688\n",
      "Step 95, loss_value: 438707.5938, style_loss: 498561.3125\n",
      "Step 96, loss_value: 434157.6250, style_loss: 498367.4375\n",
      "Step 97, loss_value: 429754.1250, style_loss: 498169.3438\n",
      "Step 98, loss_value: 425449.4688, style_loss: 497939.6250\n",
      "Step 99, loss_value: 421226.4062, style_loss: 497732.4375\n",
      "Step 100, loss_value: 417139.5625, style_loss: 497525.8125\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        loss_value, content_loss_value, style_loss_value, _ \\\n",
    "            = sess.run([loss, content_loss, style_loss, train_op], \n",
    "                      feed_dict={\n",
    "                          content: content_val,\n",
    "                          style: style_val\n",
    "                      })\n",
    "        print('Step %d, loss_value: %8.4f, style_loss: %8.4f' % (step+1, loss_value[0], content_loss_value[0]))\n",
    "        \n",
    "        result_img_path = os.path.join(output_dir, 'result_%2d.jpg' % (step+1))\n",
    "        result_val = result.eval(sess)[0]\n",
    "        result_val = np.clip(result_val, 0, 255)\n",
    "        img_arr = np.asarray(result_val, np.uint8)\n",
    "        img = Image.fromarray(img_arr)\n",
    "        img.save(result_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
