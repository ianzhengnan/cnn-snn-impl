{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Data generator\n",
    "    a. Loads vocab\n",
    "    c. Loads image features\n",
    "    d. provide data for training.\n",
    "2. Build image caption model\n",
    "3. Trains the model\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging\n",
    "import pprint\n",
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "input_description_file = './data/results.token'\n",
    "input_img_feature_dir = './data/download_inpcetion_v3_features/'\n",
    "input_vocab_file = './data/vocab.txt'\n",
    "output_dir = './data/local_run'\n",
    "\n",
    "if not gfile.Exists(output_dir):\n",
    "    gfile.MakeDirs(output_dir)\n",
    "    \n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_vocab_word_threshold = 3,\n",
    "        num_embedding_nodes = 32,\n",
    "        num_timesteps = 10,\n",
    "        num_lstm_nodes = [64,64],\n",
    "        num_lstm_layers = 2,\n",
    "        num_fc_nodes = 32,\n",
    "        batch_size = 80, \n",
    "        cell_type = 'lstm',\n",
    "        clip_lstm_grads = 1, # 梯度剪切，超过的会被设置成1\n",
    "        learning_rate = 0.001,\n",
    "        keep_prob = 0.8,\n",
    "        log_frequent = 10, # 100,  # 多久打印一次log\n",
    "        save_frequent = 100, # 1000,\n",
    "    )\n",
    "hps = get_default_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3835, 389, 1, 0]\n",
      "i have a <UNK>\n"
     ]
    }
   ],
   "source": [
    "# 载入词表\n",
    "class Vocab:\n",
    "    def __init__(self, filename, word_num_threshold):\n",
    "        self._id_to_word = {}\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._eos = -1\n",
    "        self._word_num_threshold = word_num_threshold\n",
    "        self._read_dict(filename)\n",
    "        \n",
    "    def _read_dict(self, filename):\n",
    "        with gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            word, occurrence = line.strip('\\r\\n').split('\\t')\n",
    "            occurrence = int(occurrence)\n",
    "            if occurrence < self._word_num_threshold:\n",
    "                continue\n",
    "            idx = len(self._id_to_word)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '.':\n",
    "                self._eos = idx\n",
    "            if word in self._word_to_id or idx in self._id_to_word:\n",
    "                raise Exception('duplicate words in vocab')\n",
    "            self._id_to_word[idx] = word\n",
    "            self._word_to_id[word] = idx\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "    \n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "            \n",
    "    def id_to_word(self, word_id):\n",
    "        return self._id_to_word.get(word_id, '<UNK>')\n",
    "            \n",
    "    def size(self):\n",
    "        return len(self._id_to_word)\n",
    "    \n",
    "    # 输入句子转换成词的id列表\n",
    "    def encode(self, sentence):\n",
    "        return [self.word_to_id(word) for word in sentence.split(' ')]\n",
    "             \n",
    "    # 输入id列表转换成一句话\n",
    "    def decode(self, sentence_id):\n",
    "        words = [self.id_to_word(word_id) for word_id in sentence_id]\n",
    "        return ' '.join(words)\n",
    "        \n",
    "# 测试\n",
    "vocab = Vocab(input_vocab_file, hps.num_vocab_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "encode_sentence = vocab.encode('i have a dream')\n",
    "print(encode_sentence)\n",
    "print(vocab.decode(encode_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:num of all images: 31783\n",
      "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
      " 'A little girl in a pink dress going into a wooden cabin .',\n",
      " 'A little girl climbing the stairs to her playhouse .',\n",
      " 'A little girl climbing into a wooden playhouse .',\n",
      " 'A girl going into a wooden building .']\n",
      "INFO:tensorflow:num of all images: 31783\n",
      "[[3, 52, 4, 1, 91, 117, 8, 247, 49, 1, 366, 10, 414, 4, 27, 5350, 670, 2],\n",
      " [3, 60, 30, 4, 1, 91, 117, 356, 71, 1, 227, 3610, 2],\n",
      " [3, 60, 30, 247, 5, 414, 15, 40, 3834, 2],\n",
      " [3, 60, 30, 247, 71, 1, 227, 3834, 2],\n",
      " [3, 30, 356, 71, 1, 227, 78, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 解析图片描述文件，返回(img_name, [descriptions.....])\n",
    "def parse_token_file(token_file):\n",
    "    \"\"\"parse image description file\"\"\"\n",
    "    img_name_to_tokens = {}\n",
    "    with gfile.GFile(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        img_id, description = line.strip('\\r\\n').split('\\t')\n",
    "        image_name, _ = img_id.split('#')\n",
    "        img_name_to_tokens.setdefault(image_name, [])\n",
    "        img_name_to_tokens[image_name].append(description)\n",
    "    return img_name_to_tokens\n",
    "\n",
    "def convert_token_to_id(img_name_to_tokens, vocab):\n",
    "    \"\"\"Converts token of each description of imgs to id\"\"\"\n",
    "    img_name_to_tokens_id = {}\n",
    "    for img_name in img_name_to_tokens:\n",
    "        img_name_to_tokens_id.setdefault(img_name, [])\n",
    "        for description in img_name_to_tokens[img_name]:\n",
    "            token_ids = vocab.encode(description)\n",
    "            img_name_to_tokens_id[img_name].append(token_ids)\n",
    "    return img_name_to_tokens_id\n",
    "\n",
    "img_name_to_tokens = parse_token_file(input_description_file)\n",
    "img_name_to_tokens_id = convert_token_to_id(img_name_to_tokens, vocab)\n",
    "\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_tokens))\n",
    "pprint.pprint(img_name_to_tokens['1000268201.jpg'])\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_tokens_id))\n",
    "pprint.pprint(img_name_to_tokens_id['1000268201.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/download_inpcetion_v3_features/image_features_0.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_1.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_10.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_11.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_12.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_13.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_14.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_15.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_16.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_17.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_18.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_19.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_2.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_20.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_21.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_22.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_23.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_24.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_25.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_26.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_27.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_28.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_29.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_3.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_30.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_31.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_4.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_5.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_6.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_7.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_8.pickle',\n",
      " './data/download_inpcetion_v3_features/image_features_9.pickle']\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_0.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_1.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_10.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_11.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_12.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_13.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_14.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_15.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_16.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_17.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_18.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_19.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_2.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_20.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_21.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_22.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_23.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_24.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_25.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_26.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_27.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_28.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_29.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_3.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_30.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_31.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_4.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_5.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_6.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_7.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_8.pickle\n",
      "INFO:tensorflow:loading ./data/download_inpcetion_v3_features/image_features_9.pickle\n",
      "(31783, 2048)\n",
      "(31783,)\n",
      "INFO:tensorflow:img_feature_dim: 2048\n",
      "INFO:tensorflow:caption_data_size: 31783\n",
      "array([[0.48860577, 0.17895113, 0.11176351, ..., 0.49464446, 0.6304958 ,\n",
      "        0.71422416],\n",
      "       [0.13873702, 0.06601465, 0.77972037, ..., 0.81454015, 0.39365923,\n",
      "        0.739417  ],\n",
      "       [0.21148548, 0.2699881 , 0.06800508, ..., 0.4092613 , 0.30398864,\n",
      "        0.14421895],\n",
      "       [0.31648272, 0.02064816, 0.19719888, ..., 0.8036097 , 0.67068654,\n",
      "        0.09402327],\n",
      "       [1.1104186 , 0.09665141, 0.09262174, ..., 0.31299734, 0.11267328,\n",
      "        1.0228151 ]], dtype=float32)\n",
      "array([[   3,    9,    7,   53,   18,   66, 3350,    0, 7781,  949],\n",
      "       [   3,   31,    4,    1,  177,   26,   79, 1250,   11,    1],\n",
      "       [ 180,   19,   14,   32,    6,    5,   97,    4,   38,   10],\n",
      "       [   3,   22,   33,    4,   43,    2,    2,    2,    2,    2],\n",
      "       [  16,  122,  222,   14,   35,  122,    2,    2,    2,    2]])\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])\n",
      "array(['221063801.jpg', '207216904.jpg', '6864936264.jpg',\n",
      "       '3602838407.jpg', '3163477256.jpg'], dtype='<U14')\n"
     ]
    }
   ],
   "source": [
    "class StrToBytes:\n",
    "    def __init__(self, fileobj):\n",
    "        self.fileobj = fileobj\n",
    "    def read(self, size):\n",
    "        return self.fileobj.read(size).encode()\n",
    "    def readline(self, size=-1):\n",
    "        return self.fileobj.readline(size).encode()\n",
    "\n",
    "class ImageCaptionData:\n",
    "    \"\"\"provide data for image caption model.\"\"\"\n",
    "    def __init__(self,\n",
    "                img_name_to_tokens_id, # 图片描述\n",
    "                img_feature_dir,\n",
    "                num_timesteps,\n",
    "                vocab,\n",
    "                deterministic = False):\n",
    "        self._vocab = vocab\n",
    "        self._img_name_to_tokens_id = img_name_to_tokens_id\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._deterministic = deterministic\n",
    "        self._indicator = 0\n",
    "        \n",
    "        self._img_feature_filenames = []\n",
    "        self._img_feature_data = []\n",
    "        \n",
    "        self._all_img_feature_filepaths = []\n",
    "        for filename in gfile.ListDirectory(img_feature_dir):\n",
    "            self._all_img_feature_filepaths.append(\n",
    "                os.path.join(img_feature_dir, filename))\n",
    "        pprint.pprint(self._all_img_feature_filepaths)\n",
    "        # 载入特征文件\n",
    "        self._load_img_feature_pickle()\n",
    "        \n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "        \n",
    "    def _load_img_feature_pickle(self):\n",
    "        \"\"\"load image feature data from pickle files\"\"\"\n",
    "        for filepath in self._all_img_feature_filepaths:\n",
    "            logging.info('loading %s' % filepath)\n",
    "            with gfile.GFile(filepath, 'rb') as f:\n",
    "                filenames, features = cPickle.load(f)\n",
    "                self._img_feature_filenames += filenames # 不用append.是因为filenames是一个列表。用+=可以合并两个列表\n",
    "                self._img_feature_data.append(features)\n",
    "        # [#(1000, 1, 1, 2048), #(1000, 1, 1, 2048)] -> [#(2000, 1, 1, 2048)]\n",
    "        self._img_feature_data = np.vstack(self._img_feature_data) # 合并矩阵\n",
    "        origin_shape = self._img_feature_data.shape\n",
    "        # 做一个reshape将中间的两个1去掉\n",
    "        self._img_feature_data = np.reshape(self._img_feature_data, (origin_shape[0], origin_shape[3]))\n",
    "        # 将filenames也转换成一个numpy矩阵\n",
    "        self._img_feature_filenames = np.asarray(self._img_feature_filenames)\n",
    "        print(self._img_feature_data.shape)\n",
    "        print(self._img_feature_filenames.shape)\n",
    "        \n",
    "    def size(self):\n",
    "        return len(self._img_feature_filenames)\n",
    "    \n",
    "    def img_feature_size(self):\n",
    "        return self._img_feature_data.shape[1]\n",
    "    \n",
    "    def _random_shuffle(self):\n",
    "        \"\"\"shuffle data randomly\"\"\"\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._img_feature_data = self._img_feature_data[p]\n",
    "        self._img_feature_filenames = self._img_feature_filenames[p]\n",
    "    \n",
    "    def _img_desc(self, batch_filenames):\n",
    "        \"\"\"Gets description for filename in batch\"\"\"\n",
    "        # 通过filenames知道对应的描述，然后对这些描述进行截断或者补全\n",
    "        batch_sentence_ids = []\n",
    "        batch_weights = []\n",
    "        for filename in batch_filenames:\n",
    "            token_ids_set = self._img_name_to_tokens_id[filename]\n",
    "            # 从获取的描述中随机选一个\n",
    "            chosen_token_ids = random.choice(token_ids_set)\n",
    "            chosen_token_ids_length = len(chosen_token_ids)\n",
    "            \n",
    "            weight = [1 for i in range(chosen_token_ids_length)]\n",
    "            if chosen_token_ids_length > self._num_timesteps:\n",
    "                # 做截断\n",
    "                chosen_token_ids = chosen_token_ids[0:self._num_timesteps]\n",
    "                weight = weight[0:self._num_timesteps]\n",
    "            else:\n",
    "                # 做填充\n",
    "                remaining_length = self._num_timesteps - chosen_token_ids_length\n",
    "                # 使用eos进行填充\n",
    "                chosen_token_ids += [self._vocab.eos for i in range(remaining_length)]\n",
    "                weight += [0 for i in range(remaining_length)] # 使用0填充weight\n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_weights.append(weight)\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_weights = np.asarray(batch_weights)\n",
    "        return batch_sentence_ids, batch_weights\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Returns next batch data\"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self.size():\n",
    "            if not self._deterministic:\n",
    "                self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator < self.size()\n",
    "        \n",
    "        batch_filenames = self._img_feature_filenames[self._indicator : end_indicator]\n",
    "        batch_img_features = self._img_feature_data[self._indicator : end_indicator]\n",
    "        # sentence id: [100, 101, 102, 10, 3, 0, 0, 0] -> [1,1,1,1,1,0,0,0]\n",
    "        batch_sentence_ids, batch_weights = self._img_desc(batch_filenames)\n",
    "        self._indicator = end_indicator\n",
    "        return batch_img_features, batch_sentence_ids, batch_weights, batch_filenames\n",
    "    \n",
    "caption_data = ImageCaptionData(img_name_to_tokens_id, \n",
    "                               input_img_feature_dir,\n",
    "                               hps.num_timesteps,\n",
    "                               vocab)\n",
    "\n",
    "img_feature_dim = caption_data.img_feature_size()\n",
    "caption_data_size = caption_data.size()\n",
    "logging.info('img_feature_dim: %d' % img_feature_dim)\n",
    "logging.info('caption_data_size: %d' % caption_data_size)\n",
    "\n",
    "batch_img_features, batch_sentence_ids, batch_weights, batch_img_names = caption_data.next_batch(5)\n",
    "pprint.pprint(batch_img_features)\n",
    "pprint.pprint(batch_sentence_ids)\n",
    "pprint.pprint(batch_weights)\n",
    "pprint.pprint(batch_img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-0c47ce4f2a82>:60: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From <ipython-input-5-0c47ce4f2a82>:5: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-0c47ce4f2a82>:82: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-0c47ce4f2a82>:87: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From c:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "INFO:tensorflow:variable name: embedding/embeddings:0\n",
      "INFO:tensorflow:variable name: img_feature_embed/dense/kernel:0\n",
      "INFO:tensorflow:variable name: img_feature_embed/dense/bias:0\n",
      "INFO:tensorflow:variable name: lstm_rnn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_rnn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: lstm_rnn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_rnn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/logits/kernel:0\n",
      "INFO:tensorflow:variable name: fc/logits/bias:0\n",
      "WARNING:tensorflow:From c:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    " # 返回循环神经网络的单个结构\n",
    "def create_rnn_cell(hidden_dim, cell_type):\n",
    "    \"\"\"return specific cell according to rnn type\"\"\"\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple = True)\n",
    "    elif cell_type == 'gru':\n",
    "        return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "    else:\n",
    "        raise Exception(\"%s type is not been supported\" % cell_type)\n",
    "\n",
    "# 封装dropout\n",
    "def dropout(cell, keep_prob):\n",
    "    \"\"\"wrap cell with dropout\"\"\"\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "    \n",
    "# 构建计算图\n",
    "def get_train_model(hps, vocab_size, img_feature_dim):\n",
    "    \"\"\"\n",
    "    args:\n",
    "    - hps: 参数\n",
    "    - vocab_size: 用在embedding, inference\n",
    "    - img_feature_dim: 图像特征维度\n",
    "    \"\"\"\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    \n",
    "    \n",
    "    # define placeholder\n",
    "    img_feature = tf.placeholder(tf.float32, (batch_size, img_feature_dim))\n",
    "    sentence = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    mask = tf.placeholder(tf.float32, (batch_size, num_timesteps))\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    \n",
    "    global_step = tf.Variable(tf.zeros([], tf.int32), name = 'global_step', trainable = False)\n",
    "    \n",
    "    \"\"\"\n",
    "    prediction process:\n",
    "    sentence: [a, b, c, d, e, f]\n",
    "    input: [img, a, b, c, d]\n",
    "    img_feature: [0.4, 0.3, 0.2, 0.6]\n",
    "    predict#1: img_feature -> embedding_img -> lstm -> (a)\n",
    "    predict#2: a -> embedding_word -> lstm -> (b)\n",
    "    predict#3: b -> embedding_word -> lstm -> (c)\n",
    "    .....\n",
    "    通常是将img得到的embedding_img和embedding_word合并再进行预测\n",
    "    \"\"\"\n",
    "    \n",
    "    # setup embedding layer\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0,1.0)\n",
    "    with tf.variable_scope('embedding', initializer = embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embeddings', \n",
    "            [vocab_size, hps.num_embedding_nodes], tf.float32)\n",
    "        # embed_token_ids: [batch_size, num_timestep -1, num_embedding_nodes]\n",
    "        embed_token_ids = tf.nn.embedding_lookup(\n",
    "            embeddings, \n",
    "            sentence[:, 0: num_timesteps - 1])\n",
    "    \n",
    "    # 定义一个全连接层，将图像特征变成跟分词相同的维度\n",
    "    img_feature_embed_init = tf.uniform_unit_scaling_initializer(factor = 1.0)\n",
    "    with tf.variable_scope('img_feature_embed', initializer = img_feature_embed_init):\n",
    "        # img_feature: [batch_size, img_feature_dim]\n",
    "        # embed_img: [batch_size, num_embedding_nodes]\n",
    "        embed_img = tf.keras.layers.Dense(\n",
    "            hps.num_embedding_nodes)(img_feature)\n",
    "        # embed_img: [batch_size, 1, num_embedding_nodes]\n",
    "        embed_img = tf.expand_dims(embed_img, 1)\n",
    "        # 在第一维上合并embed_img和embed_token_ids\n",
    "        # embed_inputs: [batch_size, num_timesteps, num_embedding_nodes]\n",
    "        embed_inputs = tf.concat([embed_img, embed_token_ids], axis = 1)\n",
    "        \n",
    "    # setup rnn network\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    rnn_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_rnn', initializer = rnn_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, keep_prob)\n",
    "            cells.append(cell)\n",
    "        # 合并cell\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        init_state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        # rnn_outputs: [batch_size, num_timesteps, hps.num_lstm_nodes[-1]]\n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(cell,\n",
    "                                          embed_inputs,\n",
    "                                          initial_state = init_state)\n",
    "        \n",
    "    # setup fully connected layer\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor = 0.1)\n",
    "    with tf.variable_scope('fc', initializer = fc_init):\n",
    "        rnn_outputs_2d = tf.reshape(rnn_outputs, [-1, hps.num_lstm_nodes[-1]])\n",
    "        \n",
    "        fc1 = tf.keras.layers.Dense(hps.num_fc_nodes, name = 'fc1')(rnn_outputs_2d)\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        fc1_relu = tf.nn.relu(fc1_dropout)\n",
    "        # 计算概率\n",
    "        logits = tf.keras.layers.Dense(vocab_size, name = 'logits')(fc1_relu)\n",
    "        \n",
    "        \n",
    "    # calculate loss \n",
    "    with tf.variable_scope('loss'):\n",
    "        # 展平 sentence and mask\n",
    "        sentence_flatten = tf.reshape(sentence, [-1])\n",
    "        mask_flatten = tf.reshape(mask, [-1])\n",
    "        \n",
    "        mask_sum = tf.reduce_sum(mask_flatten)\n",
    "        \n",
    "        # 计算损失\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits = logits,\n",
    "            labels = sentence_flatten )\n",
    "    \n",
    "        # 排除weight为0的值\n",
    "        weighted_softmax_loss = tf.multiply(softmax_loss, tf.cast(mask_flatten, tf.float32))\n",
    "        loss = tf.reduce_sum(weighted_softmax_loss) / mask_sum\n",
    "        \n",
    "        prediction = tf.argmax(logits, 1, output_type=tf.int32)\n",
    "        correct_prediction = tf.equal(prediction, sentence_flatten)\n",
    "        \n",
    "        weighted_correct_prediction = tf.multiply(tf.cast(correct_prediction, tf.float32), mask_flatten)\n",
    "        \n",
    "        accuracy = tf.reduce_sum(weighted_correct_prediction) / mask_sum\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        \n",
    "    # define train op\n",
    "    with tf.variable_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            logging.info('variable name: %s' % var.name)\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        optimizer = tf.train.AdadeltaOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars), global_step = global_step)\n",
    "        \n",
    "    return ((img_feature, sentence, mask, keep_prob),\n",
    "           (loss, accuracy, train_op),\n",
    "           global_step)\n",
    "\n",
    "placeholders, metrics, global_step = get_train_model(\n",
    "    hps, vocab_size, img_feature_dim)\n",
    "\n",
    "img_feature, sentence, mask, keep_prob = placeholders\n",
    "loss, accuracy, train_op = metrics\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# 用来保存模型\n",
    "saver = tf.train.Saver(max_to_keep = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:    10, loss: 9.294, accu: 0.000\n",
      "INFO:tensorflow:Step:    20, loss: 9.294, accu: 0.000\n",
      "INFO:tensorflow:Step:    30, loss: 9.294, accu: 0.000\n",
      "INFO:tensorflow:Step:    40, loss: 9.294, accu: 0.000\n",
      "INFO:tensorflow:Step:    50, loss: 9.294, accu: 0.000\n",
      "INFO:tensorflow:Step:    60, loss: 9.294, accu: 0.000\n",
      "INFO:tensorflow:Step:    70, loss: 9.294, accu: 0.000\n",
      "INFO:tensorflow:Step:    80, loss: 9.294, accu: 0.001\n",
      "INFO:tensorflow:Step:    90, loss: 9.294, accu: 0.000\n",
      "INFO:tensorflow:Step:   100, loss: 9.294, accu: 0.000\n",
      "INFO:tensorflow:Step:   100, model saved\n",
      "INFO:tensorflow:Step:   110, loss: 9.294, accu: 0.000\n",
      "INFO:tensorflow:Step:   120, loss: 9.294, accu: 0.000\n",
      "INFO:tensorflow:Step:   130, loss: 9.294, accu: 0.000\n",
      "INFO:tensorflow:Step:   140, loss: 9.294, accu: 0.000\n",
      "INFO:tensorflow:Step:   150, loss: 9.294, accu: 0.000\n",
      "INFO:tensorflow:Step:   160, loss: 9.294, accu: 0.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-55529ca3b245>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mfetches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mglobal_step_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mshould_log\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i076453\\envs\\learn-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练流程\n",
    "train_steps = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    writer = tf.summary.FileWriter(output_dir, sess.graph)\n",
    "    for i in range(train_steps):\n",
    "        (batch_img_features, batch_sentences_ids, batch_weights, _) = caption_data.next_batch(hps.batch_size)\n",
    "        input_vals = (batch_img_features, batch_sentences_ids, batch_weights, hps.keep_prob)\n",
    "        feed_dict = dict(zip(placeholders, input_vals))\n",
    "        fetches = [global_step, loss, accuracy, train_op]\n",
    "        should_log = (i + 1) % hps.log_frequent == 0\n",
    "        should_save = (i + 1) % hps.save_frequent == 0\n",
    "        \n",
    "        if should_log:\n",
    "            fetches += [summary_op]\n",
    "            \n",
    "        outputs = sess.run(fetches, feed_dict = feed_dict)\n",
    "        global_step_val, loss_val, accuracy_val = outputs[0:3]\n",
    "        if should_log:\n",
    "            summary_str = outputs[-1]\n",
    "            writer.add_summary(summary_str, global_step_val)\n",
    "            logging.info('Step: %5d, loss: %3.3f, accu: %3.3f' % (global_step_val, loss_val, accuracy_val))\n",
    "            \n",
    "        if should_save:\n",
    "            model_save_file = os.path.join(output_dir, 'image_caption')\n",
    "            logging.info('Step: %5d, model saved' % global_step_val)\n",
    "            saver.save(sess, model_save_file, global_step = global_step_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
