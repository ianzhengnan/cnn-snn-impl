{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Data generator\n",
    "    a. Loads vocab\n",
    "    c. Loads image features\n",
    "    d. provide data for training.\n",
    "2. Build image caption model\n",
    "3. Trains the model\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging\n",
    "import pprint\n",
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "input_description_file = './data/results.token'\n",
    "input_img_feature_dir = './data/download_inpcetion_v3_features/'\n",
    "input_vocab_file = './data/vocab.txt'\n",
    "output_dir = './data/local_run'\n",
    "\n",
    "if not gfile.Exists(output_dir):\n",
    "    gfile.MakeDirs(output_dir)\n",
    "    \n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_vocab_word_threshold = 3,\n",
    "        num_embedding_nodes = 32,\n",
    "        num_timesteps = 10,\n",
    "        num_lstm_nodes = [64,64],\n",
    "        num_lstm_layers = 2,\n",
    "        num_fc_nodes = 32,\n",
    "        batch_size = 80, \n",
    "        cell_type = 'lstm',\n",
    "        clip_lstm_grads = 1, # 梯度剪切，超过的会被设置成1\n",
    "        learning_rate = 0.001,\n",
    "        keep_prob = 0.8,\n",
    "        log_frequent = 10, # 100,  # 多久打印一次log\n",
    "        save_frequent = 100, # 1000,\n",
    "    )\n",
    "hps = get_default_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3835, 389, 1, 0]\n",
      "i have a <UNK>\n"
     ]
    }
   ],
   "source": [
    "# 载入词表\n",
    "class Vocab:\n",
    "    def __init__(self, filename, word_num_threshold):\n",
    "        self._id_to_word = {}\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._eos = -1\n",
    "        self._word_num_threshold = word_num_threshold\n",
    "        self._read_dict(filename)\n",
    "        \n",
    "    def _read_dict(self, filename):\n",
    "        with gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            word, occurrence = line.strip('\\r\\n').split('\\t')\n",
    "            occurrence = int(occurrence)\n",
    "            if occurrence < self._word_num_threshold:\n",
    "                continue\n",
    "            idx = len(self._id_to_word)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '.':\n",
    "                self._eos = idx\n",
    "            if word in self._word_to_id or idx in self._id_to_word:\n",
    "                raise Exception('duplicate words in vocab')\n",
    "            self._id_to_word[idx] = word\n",
    "            self._word_to_id[word] = idx\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "    \n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "            \n",
    "    def id_to_word(self, word_id):\n",
    "        return self._id_to_word.get(word_id, '<UNK>')\n",
    "            \n",
    "    def size(self):\n",
    "        return len(self._id_to_word)\n",
    "    \n",
    "    # 输入句子转换成词的id列表\n",
    "    def encode(self, sentence):\n",
    "        return [self.word_to_id(word) for word in sentence.split(' ')]\n",
    "             \n",
    "    # 输入id列表转换成一句话\n",
    "    def decode(self, sentence_id):\n",
    "        words = [self.id_to_word(word_id) for word_id in sentence_id]\n",
    "        return ' '.join(words)\n",
    "        \n",
    "# 测试\n",
    "vocab = Vocab(input_vocab_file, hps.num_vocab_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "encode_sentence = vocab.encode('i have a dream')\n",
    "print(encode_sentence)\n",
    "print(vocab.decode(encode_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:num of all images: 31783\n",
      "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
      " 'A little girl in a pink dress going into a wooden cabin .',\n",
      " 'A little girl climbing the stairs to her playhouse .',\n",
      " 'A little girl climbing into a wooden playhouse .',\n",
      " 'A girl going into a wooden building .']\n",
      "INFO:tensorflow:num of all images: 31783\n",
      "[[3, 52, 4, 1, 91, 117, 8, 247, 49, 1, 366, 10, 414, 4, 27, 5350, 670, 2],\n",
      " [3, 60, 30, 4, 1, 91, 117, 356, 71, 1, 227, 3610, 2],\n",
      " [3, 60, 30, 247, 5, 414, 15, 40, 3834, 2],\n",
      " [3, 60, 30, 247, 71, 1, 227, 3834, 2],\n",
      " [3, 30, 356, 71, 1, 227, 78, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 解析图片描述文件，返回(img_name, [descriptions.....])\n",
    "def parse_token_file(token_file):\n",
    "    \"\"\"parse image description file\"\"\"\n",
    "    img_name_to_tokens = {}\n",
    "    with gfile.GFile(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        img_id, description = line.strip('\\r\\n').split('\\t')\n",
    "        image_name, _ = img_id.split('#')\n",
    "        img_name_to_tokens.setdefault(image_name, [])\n",
    "        img_name_to_tokens[image_name].append(description)\n",
    "    return img_name_to_tokens\n",
    "\n",
    "def convert_token_to_id(img_name_to_tokens, vocab):\n",
    "    \"\"\"Converts token of each description of imgs to id\"\"\"\n",
    "    img_name_to_tokens_id = {}\n",
    "    for img_name in img_name_to_tokens:\n",
    "        img_name_to_tokens_id.setdefault(img_name, [])\n",
    "        for description in img_name_to_tokens[img_name]:\n",
    "            token_ids = vocab.encode(description)\n",
    "            img_name_to_tokens_id[img_name].append(token_ids)\n",
    "    return img_name_to_tokens_id\n",
    "\n",
    "img_name_to_tokens = parse_token_file(input_description_file)\n",
    "img_name_to_tokens_id = convert_token_to_id(img_name_to_tokens, vocab)\n",
    "\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_tokens))\n",
    "pprint.pprint(img_name_to_tokens['1000268201.jpg'])\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_tokens_id))\n",
    "pprint.pprint(img_name_to_tokens_id['1000268201.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/feature_extraction_inception_v3/image_features-0.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-1.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-10.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-11.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-12.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-13.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-14.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-15.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-16.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-17.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-18.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-19.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-2.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-20.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-21.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-22.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-23.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-24.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-25.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-26.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-27.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-28.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-29.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-3.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-30.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-31.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-4.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-5.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-6.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-7.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-8.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-9.pickle']\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-0.pickle\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "readline() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1bfd183bee35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    113\u001b[0m                                \u001b[0minput_img_feature_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                                \u001b[0mhps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m                                vocab)\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[0mimg_feature_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaption_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_feature_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-1bfd183bee35>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, img_name_to_tokens_id, img_feature_dir, num_timesteps, vocab, deterministic)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mpprint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_all_img_feature_filepaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# 载入特征文件\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_img_feature_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deterministic\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-1bfd183bee35>\u001b[0m in \u001b[0;36m_load_img_feature_pickle\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loading %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mfilenames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStrToBytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_img_feature_filenames\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[1;31m# 不用append.是因为filenames是一个列表。用+=可以合并两个列表\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_img_feature_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-1bfd183bee35>\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mImageCaptionData\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: readline() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "class StrToBytes:\n",
    "    def __init__(self, fileobj):\n",
    "        self.fileobj = fileobj\n",
    "    def read(self, size):\n",
    "        return self.fileobj.read(size).encode()\n",
    "    def readline(self, size=-1):\n",
    "        return self.fileobj.readline(size).encode()\n",
    "\n",
    "class ImageCaptionData:\n",
    "    \"\"\"provide data for image caption model.\"\"\"\n",
    "    def __init__(self,\n",
    "                img_name_to_tokens_id, # 图片描述\n",
    "                img_feature_dir,\n",
    "                num_timesteps,\n",
    "                vocab,\n",
    "                deterministic = False):\n",
    "        self._vocab = vocab\n",
    "        self._img_name_to_tokens_id = img_name_to_tokens_id\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._deterministic = deterministic\n",
    "        self._indicator = 0\n",
    "        \n",
    "        self._img_feature_filenames = []\n",
    "        self._img_feature_data = []\n",
    "        \n",
    "        self._all_img_feature_filepaths = []\n",
    "        for filename in gfile.ListDirectory(img_feature_dir):\n",
    "            self._all_img_feature_filepaths.append(\n",
    "                os.path.join(img_feature_dir, filename))\n",
    "        pprint.pprint(self._all_img_feature_filepaths)\n",
    "        # 载入特征文件\n",
    "        self._load_img_feature_pickle()\n",
    "        \n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "        \n",
    "    def _load_img_feature_pickle(self):\n",
    "        \"\"\"load image feature data from pickle files\"\"\"\n",
    "        for filepath in self._all_img_feature_filepaths:\n",
    "            logging.info('loading %s' % filepath)\n",
    "            with gfile.GFile(filepath, 'rb') as f:\n",
    "                filenames, features = cPickle.load(f)\n",
    "                self._img_feature_filenames += filenames # 不用append.是因为filenames是一个列表。用+=可以合并两个列表\n",
    "                self._img_feature_data.append(features)\n",
    "        # [#(1000, 1, 1, 2048), #(1000, 1, 1, 2048)] -> [#(2000, 1, 1, 2048)]\n",
    "        self._img_feature_data = np.vstack(self._img_feature_data) # 合并矩阵\n",
    "        origin_shape = self._img_feature_data.shape\n",
    "        # 做一个reshape将中间的两个1去掉\n",
    "        self._img_feature_data = np.reshape(self._img_feature_data, (origin_shape[0], origin_shape[3]))\n",
    "        # 将filenames也转换成一个numpy矩阵\n",
    "        self._img_feature_filenames = np.asarray(self._img_feature_filenames)\n",
    "        print(self._img_feature_data.shape)\n",
    "        print(self._img_feature_filenames.shape)\n",
    "        \n",
    "    def size(self):\n",
    "        return len(self._img_feature_filenames)\n",
    "    \n",
    "    def img_feature_size(self):\n",
    "        return self._img_feature_data.shape[1]\n",
    "    \n",
    "    def _random_shuffle(self):\n",
    "        \"\"\"shuffle data randomly\"\"\"\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._img_feature_data = self._img_feature_data[p]\n",
    "        self._img_feature_filenames = self._img_feature_filenames[p]\n",
    "    \n",
    "    def _img_desc(self, batch_filenames):\n",
    "        \"\"\"Gets description for filename in batch\"\"\"\n",
    "        # 通过filenames知道对应的描述，然后对这些描述进行截断或者补全\n",
    "        batch_sentence_ids = []\n",
    "        batch_weights = []\n",
    "        for filename in batch_filenames:\n",
    "            token_ids_set = self._img_name_to_tokens_id[filename]\n",
    "            # 从获取的描述中随机选一个\n",
    "            chosen_token_ids = random.choice(token_ids_set)\n",
    "            chosen_token_ids_length = len(chosen_token_ids)\n",
    "            \n",
    "            weight = [1 for i in range(chosen_token_ids_length)]\n",
    "            if chosen_token_ids_length > self._num_timesteps:\n",
    "                # 做截断\n",
    "                chosen_token_ids = chosen_token_ids[0:self._num_timesteps]\n",
    "                weight = weight[0:self._num_timesteps]\n",
    "            else:\n",
    "                # 做填充\n",
    "                remaining_length = self._num_timesteps - chosen_token_ids_length\n",
    "                # 使用eos进行填充\n",
    "                chosen_token_ids += [self._vocab.eos for i in range(remaining_length)]\n",
    "                weight += [0 for i in range(remaining_length)] # 使用0填充weight\n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_weights.append(weight)\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_weights = np.asarray(batch_weights)\n",
    "        return batch_sentence_ids, batch_weights\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Returns next batch data\"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self.size():\n",
    "            if not self._deterministic:\n",
    "                self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator < self.size()\n",
    "        \n",
    "        batch_filenames = self._img_feature_filenames[self._indicator : end_indicator]\n",
    "        batch_img_features = self._img_feature_data[self._indicator : end_indicator]\n",
    "        # sentence id: [100, 101, 102, 10, 3, 0, 0, 0] -> [1,1,1,1,1,0,0,0]\n",
    "        batch_sentence_ids, batch_weights = self._img_desc(batch_filenames)\n",
    "        self._indicator = end_indicator\n",
    "        return batch_img_features, batch_sentence_ids, batch_weights, batch_filenames\n",
    "    \n",
    "caption_data = ImageCaptionData(img_name_to_tokens_id, \n",
    "                               input_img_feature_dir,\n",
    "                               hps.num_timesteps,\n",
    "                               vocab)\n",
    "\n",
    "img_feature_dim = caption_data.img_feature_size()\n",
    "caption_data_size = caption_data.size()\n",
    "logging.info('img_feature_dim: %d' % img_feature_dim)\n",
    "logging.info('caption_data_size: %d' % caption_data_size)\n",
    "\n",
    "batch_img_features, batch_sentence_ids, batch_weights, batch_img_names = caption_data.next_batch(5)\n",
    "pprint.pprint(batch_img_features)\n",
    "pprint.pprint(batch_sentence_ids)\n",
    "pprint.pprint(batch_weights)\n",
    "pprint.pprint(batch_img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 返回循环神经网络的单个结构\n",
    "def create_rnn_cell(hidden_dim, cell_type):\n",
    "    \"\"\"return specific cell according to rnn type\"\"\"\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple = True)\n",
    "    elif cell_type == 'gru':\n",
    "        return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "    else:\n",
    "        raise Exception(\"%s type is not been supported\" % cell_type)\n",
    "\n",
    "# 封装dropout\n",
    "def dropout(cell, keep_prob):\n",
    "    \"\"\"wrap cell with dropout\"\"\"\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "    \n",
    "# 构建计算图\n",
    "def get_train_model(hps, vocab_size, img_feature_dim):\n",
    "    \"\"\"\n",
    "    args:\n",
    "    - hps: 参数\n",
    "    - vocab_size: 用在embedding, inference\n",
    "    - img_feature_dim: 图像特征维度\n",
    "    \"\"\"\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    \n",
    "    \n",
    "    # define placeholder\n",
    "    img_feature = tf.placeholder(tf.float32, (batch_size, img_feature_dim))\n",
    "    sentence = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    mask = tf.placeholder(tf.float32, (batch_size, num_timesteps))\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    \n",
    "    global_step = tf.Variable(tf.zeros([], tf.int32), name = 'global_step', trainable = False)\n",
    "    \n",
    "    \"\"\"\n",
    "    prediction process:\n",
    "    sentence: [a, b, c, d, e, f]\n",
    "    input: [img, a, b, c, d]\n",
    "    img_feature: [0.4, 0.3, 0.2, 0.6]\n",
    "    predict#1: img_feature -> embedding_img -> lstm -> (a)\n",
    "    predict#2: a -> embedding_word -> lstm -> (b)\n",
    "    predict#3: b -> embedding_word -> lstm -> (c)\n",
    "    .....\n",
    "    通常是将img得到的embedding_img和embedding_word合并再进行预测\n",
    "    \"\"\"\n",
    "    \n",
    "    # setup embedding layer\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0,1.0)\n",
    "    with tf.variable_scope('embedding', initializer = embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embeddings', \n",
    "            [vocab_size, hps.num_embedding_nodes], tf.float32)\n",
    "        # embed_token_ids: [batch_size, num_timestep -1, num_embedding_nodes]\n",
    "        embed_token_ids = tf.nn.embedding_lookup(\n",
    "            embeddings, \n",
    "            sentence[:, 0: num_timesteps - 1])\n",
    "    \n",
    "    # 定义一个全连接层，将图像特征变成跟分词相同的维度\n",
    "    img_feature_embed_init = tf.uniform_unit_scaling_initializer(factor = 1.0)\n",
    "    with tf.variable_scope('img_feature_embed', initializer = img_feature_embed_init):\n",
    "        # img_feature: [batch_size, img_feature_dim]\n",
    "        # embed_img: [batch_size, num_embedding_nodes]\n",
    "        embed_img = tf.keras.layers.Dense(\n",
    "            hps.num_embedding_nodes)(img_feature)\n",
    "        # embed_img: [batch_size, 1, num_embedding_nodes]\n",
    "        embed_img = tf.expand_dims(embed_img, 1)\n",
    "        # 在第一维上合并embed_img和embed_token_ids\n",
    "        # embed_inputs: [batch_size, num_timesteps, num_embedding_nodes]\n",
    "        embed_inputs = tf.concat([embed_img, embed_token_ids], axis = 1)\n",
    "        \n",
    "    # setup rnn network\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    rnn_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_rnn', initializer = rnn_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, keep_prob)\n",
    "            cells.append(cell)\n",
    "        # 合并cell\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        init_state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        # rnn_outputs: [batch_size, num_timesteps, hps.num_lstm_nodes[-1]]\n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(cell,\n",
    "                                          embed_inputs,\n",
    "                                          initial_state = init_state)\n",
    "        \n",
    "    # setup fully connected layer\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor = 0.1)\n",
    "    with tf.variable_scope('fc', initializer = fc_init):\n",
    "        rnn_outputs_2d = tf.reshape(rnn_outputs, [-1, hps.num_lstm_nodes[-1]])\n",
    "        \n",
    "        fc1 = tf.keras.layers.Dense(hps.num_fc_nodes, name = 'fc1')(rnn_outputs_2d)\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        fc1_relu = tf.nn.relu(fc1_dropout)\n",
    "        # 计算概率\n",
    "        logits = tf.keras.layers.Dense(vocab_size, name = 'logits')(fc1_relu)\n",
    "        \n",
    "        \n",
    "    # calculate loss \n",
    "    with tf.variable_scope('loss'):\n",
    "        # 展平 sentence and mask\n",
    "        sentence_flatten = tf.reshape(sentence, [-1])\n",
    "        mask_flatten = tf.reshape(mask, [-1])\n",
    "        \n",
    "        mask_sum = tf.reduce_sum(mask_flatten)\n",
    "        \n",
    "        # 计算损失\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits = logits,\n",
    "            labels = sentence_flatten )\n",
    "    \n",
    "        # 排除weight为0的值\n",
    "        weighted_softmax_loss = tf.multiply(softmax_loss, tf.cast(mask_flatten, tf.float32))\n",
    "        loss = tf.reduce_sum(weighted_softmax_loss) / mask_sum\n",
    "        \n",
    "        prediction = tf.argmax(logits, 1, output_type=tf.int32)\n",
    "        correct_prediction = tf.equal(prediction, sentence_flatten)\n",
    "        \n",
    "        weighted_correct_prediction = tf.multiply(tf.cast(correct_prediction, tf.float32), mask_flatten)\n",
    "        \n",
    "        accuracy = tf.reduce_sum(weighted_correct_prediction) / mask_sum\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        \n",
    "    # define train op\n",
    "    with tf.variable_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            logging.info('variable name: %s' % var.name)\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        optimizer = tf.train.AdadeltaOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars), global_step = global_step)\n",
    "        \n",
    "    return ((img_feature, sentence, mask, keep_prob),\n",
    "           (loss, accuracy, train_op),\n",
    "           global_step)\n",
    "\n",
    "placeholders, metrics, global_step = get_train_model(\n",
    "    hps, vocab_size, img_feature_dim)\n",
    "\n",
    "img_feature, sentence, mask, keep_prob = placeholders\n",
    "loss, accuracy, train_op = metrics\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# 用来保存模型\n",
    "saver = tf.train.Saver(max_to_keep = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练流程\n",
    "train_steps = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    writer = tf.summary.FileWriter(output_dir, sess.graph)\n",
    "    for i in range(train_steps):\n",
    "        (batch_img_features, batch_sentences_ids, batch_weights, _) = caption_data.next_batch(hps.batch_size)\n",
    "        input_vals = (batch_img_features, batch_sentences_ids, batch_weights, hps.keep_prob)\n",
    "        feed_dict = dict(zip(placeholders, input_vals))\n",
    "        fetches = [global_step, loss, accuracy, train_op]\n",
    "        should_log = (i + 1) % hps.log_frequent == 0\n",
    "        should_save = (i + 1) % hps.save_frequent == 0\n",
    "        \n",
    "        if should_log:\n",
    "            fetches += [summary_op]\n",
    "            \n",
    "        outputs = sess.run(fetches, feed_dict = feed_dict)\n",
    "        global_step_val, loss_val, accuracy_val = outputs[0:3]\n",
    "        if should_log:\n",
    "            summary_str = outputs[-1]\n",
    "            writer.add_summary(summary_str, global_step_val)\n",
    "            logging.info('Step: %5d, loss: %3.3f, accu: %3.3f' % (global_step_val, loss_val, accuracy_val))\n",
    "            \n",
    "        if should_save:\n",
    "            model_save_file = os.path.join(output_dir, 'image_caption')\n",
    "            logging.info('Step: %5d, model saved' % global_step_val)\n",
    "            saver.save(sess, model_save_file, global_step = global_step_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
